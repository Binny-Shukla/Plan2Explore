{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e25956f",
   "metadata": {},
   "source": [
    "# **P2E**\n",
    "\n",
    "Plan 2 Explore modeling learning why to chase the rewards so it will be able to learn whole env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d6a996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    " \n",
    " \n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "accae644",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action = 'ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff6330",
   "metadata": {},
   "source": [
    "## **Env Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "21b5c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 348 | Action dim: 17 | Max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Humanoid-v5')\n",
    "\n",
    "env = RescaleAction(env, min_action = -1.0, max_action = 1.0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "print(f'State dim: {state_dim} | Action dim: {action_dim} | Max action: {max_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118440a",
   "metadata": {},
   "source": [
    "## **Device Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "22dab40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "e1ea53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c2575",
   "metadata": {},
   "source": [
    "## **Imagination_Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "87bf1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.pos = 0\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Safe conversion \n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        done = safe_tensor(done)\n",
    "        \n",
    "        if state.dim() == 3:\n",
    "            state = state.squeeze(1)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            \n",
    "            self.buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.buffer[self.pos] = experience\n",
    "            self.pos = (1 + self.pos) % self.capacity\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[ind] for ind in indices])\n",
    "        \n",
    "        # Safe stacking\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612849a6",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "9f345c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "capacity = 500_000\n",
    "\n",
    "imagination_buffer = replay_buffer(capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255d1d7",
   "metadata": {},
   "source": [
    "## **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b76a1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, head_1, head_2, head_3, head_4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087632a",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a108a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Encoder(\n",
      "  (encode): Sequential(\n",
      "    (0): Linear(in_features=348, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assembly\n",
    "\n",
    "latent_dim = 256\n",
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 512\n",
    "head_4 = 256\n",
    "\n",
    "# Setup\n",
    "\n",
    "encode = Encoder(input_dim = state_dim, latent_dim = latent_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4).to(device)\n",
    "\n",
    "print('-' * 70)\n",
    "\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4167f",
   "metadata": {},
   "source": [
    "## **GRU World Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "5d02f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gru_World_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, state_dim, action_dim, head_1, head_2, head_3, head_4):\n",
    "        super(Gru_World_Model, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(latent_dim + action_dim, hidden_size = head_1, num_layers = 4, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action, h = None):\n",
    "        \n",
    "        \n",
    "        # Concat\n",
    "        \n",
    "        #print(f'Shape of state: {state.shape} | shape of action: {action.shape}')\n",
    "        \n",
    "        if state.dim() == 3:\n",
    "            \n",
    "            state = state.squeeze(1)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        if h is None:\n",
    "            \n",
    "            h = torch.zeros(4 , state.size(0), 256).to(device)\n",
    "        \n",
    "        out, h_out = self.gru(x.unsqueeze(1), h)\n",
    "        \n",
    "        out = out.squeeze(1)\n",
    "        \n",
    "        pred_state = self.fc(out)\n",
    "        \n",
    "        return pred_state, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc8b7a",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6cad6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gru_World_Model(\n",
      "  (gru): GRU(273, 256, num_layers=4, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "world_model = Gru_World_Model(latent_dim, state_dim, action_dim, head_1 = 256, head_2 = 512, head_3 = 512, head_4 = 256).to(device)\n",
    "\n",
    "print(world_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c888b0",
   "metadata": {},
   "source": [
    "## *Ensembly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "2ec61f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "    if isinstance (m, nn.Linear):\n",
    "        \n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "        if m.bias is not None:\n",
    "            \n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "5ce81946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensembly(nn.Module):\n",
    "    \n",
    "    def __init__(self, ensemble_size, base_model):\n",
    "        super(Ensembly, self).__init__()\n",
    "        \n",
    "        self.models = nn.ModuleList(\n",
    "            \n",
    "            [copy.deepcopy(base_model) for _ in range(ensemble_size)]\n",
    "        )\n",
    "        \n",
    "        for model in self.models:\n",
    "            \n",
    "            model.apply(init_weights)\n",
    "            \n",
    "        for model in self.models:\n",
    "            for param in model.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "\n",
    "            \n",
    "    def forward(self, state, action, h = None):\n",
    "        \n",
    "        preds, hidden_states = [], []\n",
    "        \n",
    "        for model in self.models:\n",
    "            \n",
    "            pred, h_out = model(state, action, h)\n",
    "            \n",
    "            preds.append(pred)\n",
    "            hidden_states.append(h_out)\n",
    "            \n",
    "        return torch.stack(preds), torch.stack(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28b87d",
   "metadata": {},
   "source": [
    "### **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "dbc2e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembly(\n",
      "  (models): ModuleList(\n",
      "    (0-3): 4 x Gru_World_Model(\n",
      "      (gru): GRU(273, 256, num_layers=4, batch_first=True)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (3): SiLU()\n",
      "        (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (5): SiLU()\n",
      "        (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_size = 4\n",
    "\n",
    "ensemble = Ensembly(ensemble_size, world_model)\n",
    "\n",
    "print(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df94584",
   "metadata": {},
   "source": [
    "### **World Model Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "7400029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class world_model_loss:\n",
    "    \n",
    "    def __init__(self, model, optimizer, scheduler, encode):\n",
    "        \n",
    "        self.encoder = encode\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.model = model\n",
    "        \n",
    "    def compute_loss(self, replay_buffer, batch_size, memory):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Conversion to latent state\n",
    "\n",
    "        \n",
    "        latent = self.encoder(states)\n",
    "        latent_next_states = self.encoder(next_states)\n",
    "        \n",
    "        \n",
    "        pred_next_latent_states, memory = self.model(latent, actions, memory)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(latent_next_states, pred_next_latent_states)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 0.5)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d07c0",
   "metadata": {},
   "source": [
    "### **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "6f0d89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "\n",
    "lr = 3e-4\n",
    "T_max = 3000\n",
    "\n",
    "# optimizers & scheduler\n",
    "\n",
    "optimizer = optim.AdamW(world_model.parameters(), lr, weight_decay = 0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)\n",
    "\n",
    "# Loss setup\n",
    "\n",
    "world_loss_function = world_model_loss(model = world_model, optimizer = optimizer, scheduler = scheduler, encode = encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095e8d7",
   "metadata": {},
   "source": [
    "## **SAC Design**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "af6e673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_size, hidden_size_2):\n",
    "        super(Feature_extractor, self).__init__()\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.feature(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305a8e7",
   "metadata": {},
   "source": [
    "### **Actor network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0903c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,action_dim, latent_dim, head_1, head_2, head_3, head_4, hidden_size, hidden_size_2, max_action = max_action):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature = Feature_extractor(input_dim = latent_dim, output_dim = head_1, hidden_size = hidden_size, hidden_size_2 = hidden_size_2)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(head_1, num_heads = 4, batch_first = True)\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # state -> feature\n",
    "        \n",
    "        feature = self.feature(state)\n",
    "        \n",
    "        # Feature -> layer norm -> Unsqueeze\n",
    "        \n",
    "        norm = self.norm(feature)\n",
    "        \n",
    "        #norm = norm.unsqueeze(1)\n",
    "        \n",
    "        # Unsqueezed norm -> MHA -> Squeeze\n",
    "        \n",
    "        attn, _ = self.mha(norm, norm, norm)\n",
    "        \n",
    "        attn = attn.squeeze(1)\n",
    "        \n",
    "        # Attn -> Actor\n",
    "        \n",
    "        x = self.actor(attn)\n",
    "        \n",
    "        # Mean and Log Std\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        log_std = self.log_std(x)\n",
    "        \n",
    "        mu = torch.tanh_(mu)                    # Range [-1.0, 1.0]\n",
    "        log_std = torch.tanh_(log_std)          # Range [-1.0, 1.0]\n",
    "        log_std = log_std.clamp(min = -5, max = 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Reparameterization Trick\n",
    "        \n",
    "        normal = torch.distributions.Normal(mu, std)\n",
    "        z = normal.rsample()\n",
    "        tanh_z = torch.tanh_(z)\n",
    "        log_prob = normal.log_prob(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        # Squashing\n",
    "        \n",
    "        squash = 2 * (torch.log(torch.tensor(2.0, device=z.device)) - z - F.softplus(-2 * z))\n",
    "        log_prob = log_prob - squash\n",
    "        log_prob = torch.sum(log_prob, dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d2ab2",
   "metadata": {},
   "source": [
    "### **Critic Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "03a2d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, head_1, head_2, head_3, head_4, hidden_size, hidden_size_2):\n",
    "        super(Critic_Network, self).__init__()\n",
    "        \n",
    "        self.feature = Feature_extractor(latent_dim + action_dim, head_1, hidden_size, hidden_size_2)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        self.norm_1 = nn.LayerNorm(head_1)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(head_1, num_heads = 4, batch_first = True)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "        self.critic_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        cat = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        # Feature -> layer norm -> Unsqueeze\n",
    "        \n",
    "        feature = self.feature(cat)\n",
    "\n",
    "        norm = self.norm(feature)\n",
    "        norm_1 = self.norm_1(feature)\n",
    "        \n",
    "        norm = norm.unsqueeze(1)\n",
    "        norm_1 = norm_1.unsqueeze(1)\n",
    "        \n",
    "        # Unsqueezed norm -> MHA -> Squeeze\n",
    "        \n",
    "        attn, _ = self.mha(norm, norm, norm)\n",
    "        attn_1, _ = self.mha(norm_1, norm_1, norm_1)         # Used same MHA Layer due to computation\n",
    "        \n",
    "        attn = attn.squeeze(1)\n",
    "        attn_1 = attn_1.squeeze(1)\n",
    "        \n",
    "        q_1 = self.critic(attn)\n",
    "        q_2 = self.critic_2(attn_1)\n",
    "        \n",
    "        return q_1, q_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfbb23",
   "metadata": {},
   "source": [
    "### **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "abff1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor_Network(\n",
      "  (feature): Feature_extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): SiLU()\n",
      "      (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (7): SiLU()\n",
      "      (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (9): SiLU()\n",
      "      (10): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=256, out_features=17, bias=True)\n",
      "  (log_std): Linear(in_features=256, out_features=17, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Critic_Network(\n",
      "  (feature): Feature_extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=273, out_features=256, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): SiLU()\n",
      "      (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (7): SiLU()\n",
      "      (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (9): SiLU()\n",
      "      (10): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (critic_2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size = 256\n",
    "hidden_size_2 = 512\n",
    "\n",
    "\n",
    "# Actor setup\n",
    "\n",
    "actor_network = Actor_Network(action_dim, latent_dim, head_1, head_2, head_3, head_4, hidden_size, hidden_size_2).to(device)\n",
    "\n",
    "print(actor_network)\n",
    "\n",
    "# Critic setup\n",
    "\n",
    "critic_network = Critic_Network(latent_dim, action_dim, head_1, head_2, head_3, head_4, hidden_size, hidden_size_2).to(device)\n",
    "\n",
    "print('-' * 70)\n",
    "\n",
    "print(critic_network)\n",
    "\n",
    "# Target critic\n",
    "\n",
    "target_critic = copy.deepcopy(critic_network).to(device)\n",
    "\n",
    "# Target actor\n",
    "target_actor = copy.deepcopy(actor_network).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e230c",
   "metadata": {},
   "source": [
    "## **Agent Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1100eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sac_Agent:\n",
    "    \n",
    "    def __init__(self, action_dim, actor_network, critic_network, target_critic, actor_opt, critic_opt, actor_sch, critic_sch, gamma, tau, world_loss_function, beta, encode = encode, target_actor = target_actor):\n",
    "        \n",
    "        # Network\n",
    "        \n",
    "        self.actor = actor_network\n",
    "        self.critic = critic_network\n",
    "        self.target_critic = target_critic\n",
    "        self.target_actor = target_actor\n",
    "        self.encode = encode\n",
    "        \n",
    "        # Opt and Sch\n",
    "        \n",
    "        self.actor_opt = actor_opt\n",
    "        self.actor_sch = actor_sch\n",
    "        \n",
    "        self.critic_opt = critic_opt\n",
    "        self.critic_sch = critic_sch\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.world_loss_function = world_loss_function\n",
    "        \n",
    "        self.target_entropy = - action_dim * 1.5\n",
    "        self.log_alpha = torch.tensor(np.log(0.3), requires_grad = True, device = device)\n",
    "        self.alpha_min = 0.1\n",
    "        self.alpha_opt = optim.AdamW([self.log_alpha], lr = 1e-4, weight_decay = 0.001)\n",
    "        \n",
    "    \n",
    "    def soft_update(self, source, target, tau):\n",
    "        \n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            \n",
    "            param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "\n",
    "    def select_action(self, state):\n",
    "        \n",
    "        latent = self.encode(state)\n",
    "        \n",
    "        action, log_prob = self.actor(latent)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    \n",
    "    def compute_alpha(self):\n",
    "        \n",
    "        alpha = self.log_alpha.exp().detach()\n",
    "        alpha = alpha.clamp(self.alpha_min, 0.3)\n",
    "        return alpha\n",
    "    \n",
    "    def update(self, replay_buffer, batch_size):\n",
    "                \n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        states = states.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Compute target vals\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #latent_next_states = self.encode(next_states)\n",
    "            \n",
    "            next_action, _ = self.actor(next_states)\n",
    "            \n",
    "            target_1, target_2 = self.target_critic(next_states, next_action)\n",
    "            target = (0.75 * torch.min(target_1, target_2) + 0.25 * torch.max(target_1, target_2))\n",
    "            target = target.detach()\n",
    "            \n",
    "            target_qvals = rewards + self.gamma * (1 - dones) * target\n",
    "        \n",
    "        \n",
    "        # compute critic loss\n",
    "        \n",
    "        #latent_states = self.encode(states)\n",
    "        \n",
    "        new_action, log_prob = self.actor(states)\n",
    "        \n",
    "        critic_1, critic_2 = self.critic(states, new_action)\n",
    "        critic_loss_1 = F.smooth_l1_loss(critic_1, target_qvals)\n",
    "        critic_loss_2 = F.smooth_l1_loss(critic_2, target_qvals)\n",
    "        \n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        \n",
    "        # Update critic\n",
    "        \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm = 0.5)\n",
    "        self.critic_opt.step()\n",
    "        self.critic_sch.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        \n",
    "        #latent_states_2 = self.encode(states)\n",
    "        \n",
    "        n_action, n_log_prob = self.actor(states)\n",
    "        q_1, q_2 = self.critic(states, n_action)\n",
    "        q_pi = (0.75 * torch.min(q_1, q_2) + 0.25 * torch.max(q_1, q_2)).mean()\n",
    "        actor_loss = -(self.compute_alpha() * n_log_prob - q_pi).mean()      \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            _ ,log_prob = self.target_actor(states)\n",
    "            log_prob = log_prob.detach()\n",
    "            \n",
    "        kl_div = (n_log_prob - log_prob).mean()\n",
    "        \n",
    "        \n",
    "        actor_loss += self.beta * kl_div\n",
    "\n",
    "        # Update actor\n",
    "        \n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm = 0.5)\n",
    "        self.actor_opt.step()\n",
    "        self.actor_sch.step()\n",
    "        \n",
    "        # Compute alpha loss\n",
    "        \n",
    "        alpha_loss = -(self.log_alpha * (self.target_entropy + n_log_prob.detach())).mean()\n",
    "        \n",
    "        # Update alpha\n",
    "        \n",
    "        self.alpha_opt.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm = 0.5)\n",
    "        self.alpha_opt.step()\n",
    "        \n",
    "        self.soft_update(self.critic, self.target_critic, self.tau)\n",
    "        self.soft_update(self.actor, self.target_actor, self.tau)\n",
    "        \n",
    "        self.alpha = self.compute_alpha()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(),  self.alpha.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00c82e",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "95458391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "\n",
    "gamma = 0.997\n",
    "tau = 0.067\n",
    "\n",
    "# Opt and Sch\n",
    "\n",
    "actor_optimizer = optim.AdamW(actor_network.parameters(), lr, weight_decay = 0.001)\n",
    "actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)\n",
    "\n",
    "critic_optimizer = optim.AdamW(critic_network.parameters(), lr, weight_decay = 0.001)\n",
    "critic_scheduler = optim.lr_scheduler.CosineAnnealingLR(critic_optimizer, T_max)\n",
    "\n",
    "# Agent setup\n",
    "\n",
    "sac_agent = Sac_Agent(action_dim,\n",
    "                      actor_network,\n",
    "                      critic_network,\n",
    "                      target_critic,\n",
    "                      actor_optimizer,\n",
    "                      critic_optimizer,\n",
    "                      actor_scheduler,\n",
    "                      critic_scheduler,\n",
    "                      gamma,\n",
    "                      tau,\n",
    "                      beta = 0.6,\n",
    "                      world_loss_function = world_loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758fa4b",
   "metadata": {},
   "source": [
    "## **Intrinsic reward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "faaed22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_reward_function(ensemble, encoder, state, action):\n",
    "    \n",
    "    latent = encoder(state)\n",
    "    \n",
    "    preds, hidden_memory = ensemble(latent, action)\n",
    "    \n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    \n",
    "    if preds.dim() == 3:                                         # shape [batch, ensemble_size, latent]\n",
    "        \n",
    "        disagreement = torch.var(preds, dim = 1)\n",
    "    \n",
    "        intrinsic_reward = torch.sum(disagreement, dim = 1, keepdim = True)\n",
    "    \n",
    "    elif preds.dim() == 2:                                        # shape [ensemble_size, latent]\n",
    "    \n",
    "        disagreement = torch.var(preds, dim = 0)\n",
    "        \n",
    "        intrinsic_reward = torch.sum(disagreement, dim = 0)\n",
    "    \n",
    "    return intrinsic_reward * 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49412b4d",
   "metadata": {},
   "source": [
    "### **SAC Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d522cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_BUFFER:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "        self.pos = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Safe conversion to tensor\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        done = safe_tensor(done)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            \n",
    "            self.buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.buffer[self.pos] = experience\n",
    "            self.pos = (1 + self.pos) % self.capacity\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[ind] for ind in indices])\n",
    "        \n",
    "        # Safe conversion\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        if states.dim() == 3:\n",
    "            \n",
    "            states = states.squeeze(1)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0ba1c",
   "metadata": {},
   "source": [
    "### **Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "93847284",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_buffer = SAC_BUFFER(capacity = 500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076b5ed",
   "metadata": {},
   "source": [
    "## **RollOut Trajectories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a7cc2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(world_model, agent, batch_size, ensemble = ensemble, horizon_length = 7, sac_buffer = sac_buffer, encoder = encode, imagination_buffer = imagination_buffer):\n",
    "    \n",
    "    states, _, _, _, _ = sac_buffer.sample(batch_size)\n",
    "    \n",
    "    latent = encoder(states)\n",
    "    \n",
    "    h = None\n",
    "    \n",
    "    total_intrinsive_reward = 0.0\n",
    "    \n",
    "    for length in range(horizon_length):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            action, _ = agent.select_action(states)\n",
    "            \n",
    "            next_states, h = world_model(latent, action, h)\n",
    "            \n",
    "            intrinsic_reward = intrinsic_reward_function(ensemble, encoder, states, action)   \n",
    "            total_intrinsive_reward += intrinsic_reward.max().item()\n",
    "            \n",
    "            dones = torch.zeros_like(intrinsic_reward)\n",
    "            \n",
    "            imagination_buffer.add(states, action, intrinsic_reward, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            return total_intrinsive_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2ddb7",
   "metadata": {},
   "source": [
    "## **Mix Batch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix_Batch:\n",
    "    \n",
    "    def __init__(self, ratio, imagination_buffer = imagination_buffer, sac_buffer = sac_buffer, encode = encode):\n",
    "        \n",
    "        self.ratio = ratio\n",
    "        self.sac_buffer = sac_buffer\n",
    "        self.imagination_buffer = imagination_buffer\n",
    "        self.encode = encode\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        size = int(self.ratio * batch_size)\n",
    "        actual_batch = batch_size - size\n",
    "        \n",
    "        # Sample\n",
    "        \n",
    "        i_s, i_a, i_r, i_ns, i_d = self.imagination_buffer.sample(actual_batch)\n",
    "        \n",
    "        s, a , r, ns, d = self.sac_buffer.sample(size)\n",
    "                \n",
    "        i_s = i_s[:, -1, :]\n",
    "        i_a = i_a[:, -1, :]\n",
    "        i_ns = i_ns[:, -1, :]\n",
    "        i_d = i_d[:, -1, :]\n",
    "        i_r = i_r[:, -1, :]\n",
    "        \n",
    "        r = r.view(-1, 1)\n",
    "        d = d.view(-1, 1)       \n",
    "        \n",
    "        \n",
    "        ns = self.encode(ns)\n",
    "        s = self.encode(s)\n",
    "        i_s = self.encode(i_s)\n",
    "        \n",
    "        # Now concate\n",
    "        \n",
    "        states = torch.cat([s, i_s], dim = 0).to(device)\n",
    "        actions = torch.cat([a, i_a], dim = 0).to(device)\n",
    "        rewards = torch.cat([r, i_r], dim = 0).to(device)\n",
    "        next_states = torch.cat([ns, i_ns], dim = 0).to(device)\n",
    "        dones = torch.cat([d, i_d], dim = 0).to(device)\n",
    "        \n",
    "        \n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "dab41bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.25\n",
    "\n",
    "# Setup\n",
    "\n",
    "mix_batch = Mix_Batch(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8772c8",
   "metadata": {},
   "source": [
    "| Tensor          | Shape               |\n",
    "|-----------------|---------------------|\n",
    "| Raw State       | [B, state_dim=17]   |\n",
    "| Encoded State   | [B, latent_dim=256] |\n",
    "| GRU Input       | [B, 1, 256+6]       |\n",
    "| GRU Hidden      | [2, B, 128]         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e136f98",
   "metadata": {},
   "source": [
    "## **Training Block**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "47aca9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaped_reward(reward , state):\n",
    "    \n",
    "    is_falling = (state[2] < 0.8 or          # Height threshold\n",
    "                 abs(state[3]) > 0.4 or      # Angular velocity X\n",
    "                 abs(state[4]) > 0.4)        # Angular velocity Y\n",
    "    \n",
    "    if is_falling:\n",
    "        \n",
    "        recovery_bonus = 3.0 * (1 - abs(state[3]))\n",
    "        survival_bonus = 0.5 * (0.8 - state[2])\n",
    "        reward += recovery_bonus + survival_bonus\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        reward += 0.05\n",
    "        reward += 1.5 * state[1]\n",
    "        reward += 0.3 * state[2] ** 2 # Height bonus quadritic\n",
    "        reward += 1.5 * state[0] * (1 + state[2])   # Forward locomotion\n",
    "        reward -= 0.1 * np.square(state[3:6]).sum()  # Angular penalty\n",
    "        reward -= 0.1 * (state[3] ** 2 + state[4] ** 2)\n",
    "        \n",
    "    return reward / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "02a7ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_block(env, agent, world_model, world_loss_function,\n",
    "                   sac_buffer, mix_batch, max_episodes,\n",
    "                   world_optimizer, world_scheduler, encoder,\n",
    "                   batch_size, roll_out_break, memory, warm_up):\n",
    "\n",
    "    world_loss, actor_loss, critic_loss, alpha = 0, 0, 0, 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        ep_intrinsive = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Prepare state\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Select and execute action\n",
    "            \n",
    "            action, _ = agent.select_action(state)\n",
    "            action = action.detach().cpu().numpy()[0]\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Store transition\n",
    "            \n",
    "            reward += shaped_reward(reward, state.squeeze(0))\n",
    "            sac_buffer.add(state, action, reward, next_state, float(done))\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Skip training until buffer is warm\n",
    "        if episode <= warm_up or len(sac_buffer) < batch_size:\n",
    "            print(f\"Episode: {episode} | Reward: {ep_reward:.2f} | Warm-up phase...\")\n",
    "            continue\n",
    "\n",
    "        # Train world model\n",
    "        world_loss = world_loss_function.compute_loss(sac_buffer, batch_size, memory)\n",
    "\n",
    "        # Perform imagination rollouts at fixed intervals\n",
    "        if episode % roll_out_break == 0:\n",
    "            \n",
    "            intrinsive_reward = roll_out(world_model, agent, batch_size)\n",
    "            ep_intrinsive += intrinsive_reward\n",
    "\n",
    "            \n",
    "            print(f\"Episode: {episode} | Intrinsic reward: {ep_intrinsive}\")\n",
    "            \n",
    "            \n",
    "        # Update SAC with mixed buffer—only if imagination is ready\n",
    "        if len(mix_batch.imagination_buffer) >= batch_size:\n",
    "            actor_loss, critic_loss, alpha = agent.update(mix_batch, batch_size)\n",
    "        #else:\n",
    "            #actor_loss, critic_loss, alpha = agent.update(sac_buffer, batch_size)\n",
    "\n",
    "        # Logging\n",
    "        print(\" - \" * 30)\n",
    "        print(f\"Episode: {episode} | Reward: {ep_reward:.2f}  \"\n",
    "              f\"Actor Loss: {actor_loss:.4f} | Critic Loss: {critic_loss:.4f} | Alpha: {alpha:.4f}\")\n",
    "        print(f\"World Loss: {world_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "0b81da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Reward: 91.21 | Warm-up phase...\n",
      "Episode: 2 | Reward: 162.63 | Warm-up phase...\n",
      "Episode: 3 | Reward: 112.28 | Warm-up phase...\n",
      "Episode: 4 | Reward: 91.38 | Warm-up phase...\n",
      "Episode: 5 | Reward: 89.37 | Warm-up phase...\n",
      "Episode: 6 | Reward: 91.11 | Warm-up phase...\n",
      "Episode: 7 | Reward: 90.45 | Warm-up phase...\n",
      "Episode: 8 | Reward: 103.72 | Warm-up phase...\n",
      "Episode: 9 | Reward: 127.68 | Warm-up phase...\n",
      "Episode: 10 | Reward: 104.48 | Warm-up phase...\n",
      "Episode: 11 | Reward: 84.89 | Warm-up phase...\n",
      "Episode: 12 | Reward: 113.31 | Warm-up phase...\n",
      "Episode: 13 | Reward: 109.25 | Warm-up phase...\n",
      "Episode: 14 | Intrinsic reward: 0.3629230260848999\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 14 | Reward: 85.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0501\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 15 | Reward: 123.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0517\n",
      "\n",
      "Episode: 16 | Intrinsic reward: 0.36260730028152466\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 16 | Reward: 91.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0310\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 17 | Reward: 91.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0332\n",
      "\n",
      "Episode: 18 | Intrinsic reward: 0.3586592674255371\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 18 | Reward: 112.12  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0291\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 19 | Reward: 100.90  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0456\n",
      "\n",
      "Episode: 20 | Intrinsic reward: 0.37164023518562317\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 20 | Reward: 96.83  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0262\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 21 | Reward: 90.47  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0390\n",
      "\n",
      "Episode: 22 | Intrinsic reward: 0.37119001150131226\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 22 | Reward: 122.87  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0228\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 23 | Reward: 90.58  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0552\n",
      "\n",
      "Episode: 24 | Intrinsic reward: 0.36087194085121155\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 24 | Reward: 101.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0417\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 25 | Reward: 111.54  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0557\n",
      "\n",
      "Episode: 26 | Intrinsic reward: 0.3701971173286438\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 26 | Reward: 96.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0428\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 27 | Reward: 97.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0213\n",
      "\n",
      "Episode: 28 | Intrinsic reward: 0.36647361516952515\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 28 | Reward: 91.56  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0256\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 29 | Reward: 89.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0223\n",
      "\n",
      "Episode: 30 | Intrinsic reward: 0.37226080894470215\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 30 | Reward: 137.25  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0359\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 31 | Reward: 89.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0336\n",
      "\n",
      "Episode: 32 | Intrinsic reward: 0.37086156010627747\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 32 | Reward: 106.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0370\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 33 | Reward: 102.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0545\n",
      "\n",
      "Episode: 34 | Intrinsic reward: 0.3701622486114502\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 34 | Reward: 96.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0321\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 35 | Reward: 91.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      "Episode: 36 | Intrinsic reward: 0.36177900433540344\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 36 | Reward: 90.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0234\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 37 | Reward: 84.45  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0288\n",
      "\n",
      "Episode: 38 | Intrinsic reward: 0.36955416202545166\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 38 | Reward: 97.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0289\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 39 | Reward: 109.42  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0297\n",
      "\n",
      "Episode: 40 | Intrinsic reward: 0.3719155788421631\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 40 | Reward: 89.56  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0368\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 41 | Reward: 90.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0326\n",
      "\n",
      "Episode: 42 | Intrinsic reward: 0.3703896105289459\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 42 | Reward: 231.25  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0259\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 43 | Reward: 103.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0331\n",
      "\n",
      "Episode: 44 | Intrinsic reward: 0.37132468819618225\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 44 | Reward: 85.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0480\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 45 | Reward: 91.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0339\n",
      "\n",
      "Episode: 46 | Intrinsic reward: 0.372499942779541\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 46 | Reward: 112.37  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 47 | Reward: 101.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0189\n",
      "\n",
      "Episode: 48 | Intrinsic reward: 0.37081560492515564\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 48 | Reward: 89.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0214\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 49 | Reward: 114.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0335\n",
      "\n",
      "Episode: 50 | Intrinsic reward: 0.3566339910030365\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 50 | Reward: 90.91  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0213\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 51 | Reward: 85.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0334\n",
      "\n",
      "Episode: 52 | Intrinsic reward: 0.3712385296821594\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 52 | Reward: 108.58  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0276\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 53 | Reward: 85.54  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0146\n",
      "\n",
      "Episode: 54 | Intrinsic reward: 0.3608940839767456\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 54 | Reward: 90.99  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0284\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 55 | Reward: 107.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0317\n",
      "\n",
      "Episode: 56 | Intrinsic reward: 0.37172597646713257\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 56 | Reward: 196.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0267\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 57 | Reward: 108.39  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0244\n",
      "\n",
      "Episode: 58 | Intrinsic reward: 0.35750943422317505\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 58 | Reward: 99.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0317\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 59 | Reward: 96.88  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0226\n",
      "\n",
      "Episode: 60 | Intrinsic reward: 0.359261691570282\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 60 | Reward: 107.96  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0218\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 61 | Reward: 85.81  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0265\n",
      "\n",
      "Episode: 62 | Intrinsic reward: 0.3665042817592621\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 62 | Reward: 102.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0388\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 63 | Reward: 124.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0223\n",
      "\n",
      "Episode: 64 | Intrinsic reward: 0.36200106143951416\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 64 | Reward: 91.20  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0254\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 65 | Reward: 91.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0264\n",
      "\n",
      "Episode: 66 | Intrinsic reward: 0.37429705262184143\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 66 | Reward: 89.99  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0375\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 67 | Reward: 117.64  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0331\n",
      "\n",
      "Episode: 68 | Intrinsic reward: 0.36121734976768494\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 68 | Reward: 96.43  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0436\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 69 | Reward: 96.35  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0329\n",
      "\n",
      "Episode: 70 | Intrinsic reward: 0.3651577830314636\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 70 | Reward: 96.94  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0346\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 71 | Reward: 89.30  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      "Episode: 72 | Intrinsic reward: 0.3722306191921234\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 72 | Reward: 96.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0384\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 73 | Reward: 120.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0214\n",
      "\n",
      "Episode: 74 | Intrinsic reward: 0.3660282492637634\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 74 | Reward: 109.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0411\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 75 | Reward: 91.06  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 76 | Intrinsic reward: 0.3734968900680542\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 76 | Reward: 109.35  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0266\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 77 | Reward: 160.46  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0357\n",
      "\n",
      "Episode: 78 | Intrinsic reward: 0.3654014468193054\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 78 | Reward: 97.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0182\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 79 | Reward: 108.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0273\n",
      "\n",
      "Episode: 80 | Intrinsic reward: 0.3724637031555176\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 80 | Reward: 103.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0341\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 81 | Reward: 90.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0251\n",
      "\n",
      "Episode: 82 | Intrinsic reward: 0.3704695701599121\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 82 | Reward: 115.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0388\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 83 | Reward: 112.38  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0294\n",
      "\n",
      "Episode: 84 | Intrinsic reward: 0.37115103006362915\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 84 | Reward: 91.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0295\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 85 | Reward: 115.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0293\n",
      "\n",
      "Episode: 86 | Intrinsic reward: 0.3656889498233795\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 86 | Reward: 125.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0340\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 87 | Reward: 100.67  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0367\n",
      "\n",
      "Episode: 88 | Intrinsic reward: 0.3720282316207886\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 88 | Reward: 91.20  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0151\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 89 | Reward: 152.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 90 | Intrinsic reward: 0.3677828311920166\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 90 | Reward: 101.14  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0203\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 91 | Reward: 85.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0302\n",
      "\n",
      "Episode: 92 | Intrinsic reward: 0.37063276767730713\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 92 | Reward: 90.30  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0249\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 93 | Reward: 113.40  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0170\n",
      "\n",
      "Episode: 94 | Intrinsic reward: 0.36571046710014343\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 94 | Reward: 156.54  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0346\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 95 | Reward: 97.23  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0296\n",
      "\n",
      "Episode: 96 | Intrinsic reward: 0.35908961296081543\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 96 | Reward: 153.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0381\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 97 | Reward: 174.62  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0333\n",
      "\n",
      "Episode: 98 | Intrinsic reward: 0.35949984192848206\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 98 | Reward: 131.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0208\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 99 | Reward: 106.78  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0268\n",
      "\n",
      "Episode: 100 | Intrinsic reward: 0.3714712858200073\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 100 | Reward: 116.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0322\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 101 | Reward: 104.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0296\n",
      "\n",
      "Episode: 102 | Intrinsic reward: 0.36720144748687744\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 102 | Reward: 89.91  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0309\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 103 | Reward: 84.78  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 104 | Intrinsic reward: 0.3662506937980652\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 104 | Reward: 108.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0516\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 105 | Reward: 114.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0385\n",
      "\n",
      "Episode: 106 | Intrinsic reward: 0.36306965351104736\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 106 | Reward: 174.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0278\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 107 | Reward: 91.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0258\n",
      "\n",
      "Episode: 108 | Intrinsic reward: 0.37004420161247253\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 108 | Reward: 85.21  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0289\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 109 | Reward: 91.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0220\n",
      "\n",
      "Episode: 110 | Intrinsic reward: 0.3722211420536041\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 110 | Reward: 180.98  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0329\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 111 | Reward: 91.43  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0377\n",
      "\n",
      "Episode: 112 | Intrinsic reward: 0.3644256889820099\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 112 | Reward: 113.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0209\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 113 | Reward: 103.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0299\n",
      "\n",
      "Episode: 114 | Intrinsic reward: 0.36914193630218506\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 114 | Reward: 90.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0366\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 115 | Reward: 108.98  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0205\n",
      "\n",
      "Episode: 116 | Intrinsic reward: 0.3712788224220276\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 116 | Reward: 91.46  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 117 | Reward: 91.38  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0318\n",
      "\n",
      "Episode: 118 | Intrinsic reward: 0.36051738262176514\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 118 | Reward: 96.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0262\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 119 | Reward: 97.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0301\n",
      "\n",
      "Episode: 120 | Intrinsic reward: 0.36280596256256104\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 120 | Reward: 111.41  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0204\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 121 | Reward: 119.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0339\n",
      "\n",
      "Episode: 122 | Intrinsic reward: 0.36931952834129333\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 122 | Reward: 88.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0273\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 123 | Reward: 91.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0277\n",
      "\n",
      "Episode: 124 | Intrinsic reward: 0.37159091234207153\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 124 | Reward: 90.98  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0247\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 125 | Reward: 111.42  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0359\n",
      "\n",
      "Episode: 126 | Intrinsic reward: 0.3690483868122101\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 126 | Reward: 104.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0359\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 127 | Reward: 91.43  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0338\n",
      "\n",
      "Episode: 128 | Intrinsic reward: 0.3717162311077118\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 128 | Reward: 102.75  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0245\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 129 | Reward: 97.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0344\n",
      "\n",
      "Episode: 130 | Intrinsic reward: 0.3717319965362549\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 130 | Reward: 95.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0385\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 131 | Reward: 108.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0358\n",
      "\n",
      "Episode: 132 | Intrinsic reward: 0.35826757550239563\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 132 | Reward: 97.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0413\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 133 | Reward: 82.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0320\n",
      "\n",
      "Episode: 134 | Intrinsic reward: 0.36126428842544556\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 134 | Reward: 130.10  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0201\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 135 | Reward: 97.40  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0366\n",
      "\n",
      "Episode: 136 | Intrinsic reward: 0.371957927942276\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 136 | Reward: 85.62  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0196\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 137 | Reward: 120.31  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 138 | Intrinsic reward: 0.36966636776924133\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 138 | Reward: 90.78  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0283\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 139 | Reward: 97.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0191\n",
      "\n",
      "Episode: 140 | Intrinsic reward: 0.3683316111564636\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 140 | Reward: 85.47  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0285\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 141 | Reward: 126.73  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0284\n",
      "\n",
      "Episode: 142 | Intrinsic reward: 0.3599890470504761\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 142 | Reward: 85.80  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0420\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 143 | Reward: 91.02  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0305\n",
      "\n",
      "Episode: 144 | Intrinsic reward: 0.3701915144920349\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 144 | Reward: 96.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0350\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 145 | Reward: 97.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0282\n",
      "\n",
      "Episode: 146 | Intrinsic reward: 0.37071895599365234\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 146 | Reward: 174.83  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0199\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 147 | Reward: 102.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0331\n",
      "\n",
      "Episode: 148 | Intrinsic reward: 0.3696267604827881\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 148 | Reward: 88.88  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0287\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 149 | Reward: 103.02  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0418\n",
      "\n",
      "Episode: 150 | Intrinsic reward: 0.3624821603298187\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 150 | Reward: 83.42  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0232\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 151 | Reward: 103.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0271\n",
      "\n",
      "Episode: 152 | Intrinsic reward: 0.3676266372203827\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 152 | Reward: 96.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0237\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 153 | Reward: 115.56  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0266\n",
      "\n",
      "Episode: 154 | Intrinsic reward: 0.3729504346847534\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 154 | Reward: 157.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0137\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 155 | Reward: 147.75  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0267\n",
      "\n",
      "Episode: 156 | Intrinsic reward: 0.35904115438461304\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 156 | Reward: 97.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0231\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 157 | Reward: 96.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0267\n",
      "\n",
      "Episode: 158 | Intrinsic reward: 0.361437052488327\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 158 | Reward: 199.40  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0447\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 159 | Reward: 91.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0243\n",
      "\n",
      "Episode: 160 | Intrinsic reward: 0.36762261390686035\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 160 | Reward: 91.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0218\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 161 | Reward: 91.12  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0248\n",
      "\n",
      "Episode: 162 | Intrinsic reward: 0.36709359288215637\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 162 | Reward: 96.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0401\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 163 | Reward: 97.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0316\n",
      "\n",
      "Episode: 164 | Intrinsic reward: 0.36959800124168396\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 164 | Reward: 120.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0268\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 165 | Reward: 91.18  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      "Episode: 166 | Intrinsic reward: 0.37283697724342346\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 166 | Reward: 174.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0407\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 167 | Reward: 85.39  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0301\n",
      "\n",
      "Episode: 168 | Intrinsic reward: 0.3615990877151489\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 168 | Reward: 123.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0271\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 169 | Reward: 103.80  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0263\n",
      "\n",
      "Episode: 170 | Intrinsic reward: 0.37240707874298096\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 170 | Reward: 136.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0347\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 171 | Reward: 97.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0425\n",
      "\n",
      "Episode: 172 | Intrinsic reward: 0.3574243485927582\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 172 | Reward: 89.10  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0355\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 173 | Reward: 98.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0232\n",
      "\n",
      "Episode: 174 | Intrinsic reward: 0.36657798290252686\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 174 | Reward: 95.99  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0285\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 175 | Reward: 96.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0278\n",
      "\n",
      "Episode: 176 | Intrinsic reward: 0.37041759490966797\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 176 | Reward: 104.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0201\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 177 | Reward: 91.14  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0226\n",
      "\n",
      "Episode: 178 | Intrinsic reward: 0.37386050820350647\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 178 | Reward: 84.35  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0371\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 179 | Reward: 85.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0354\n",
      "\n",
      "Episode: 180 | Intrinsic reward: 0.3695783317089081\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 180 | Reward: 118.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0239\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 181 | Reward: 165.53  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0348\n",
      "\n",
      "Episode: 182 | Intrinsic reward: 0.36784684658050537\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 182 | Reward: 110.40  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0257\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 183 | Reward: 147.17  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0244\n",
      "\n",
      "Episode: 184 | Intrinsic reward: 0.37230563163757324\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 184 | Reward: 91.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0201\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 185 | Reward: 108.83  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0326\n",
      "\n",
      "Episode: 186 | Intrinsic reward: 0.36568737030029297\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 186 | Reward: 118.74  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0271\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 187 | Reward: 90.83  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      "Episode: 188 | Intrinsic reward: 0.3715665936470032\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 188 | Reward: 84.14  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0295\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 189 | Reward: 129.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0356\n",
      "\n",
      "Episode: 190 | Intrinsic reward: 0.36983633041381836\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 190 | Reward: 91.08  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0215\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 191 | Reward: 127.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0211\n",
      "\n",
      "Episode: 192 | Intrinsic reward: 0.37206196784973145\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 192 | Reward: 90.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0325\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 193 | Reward: 102.81  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0177\n",
      "\n",
      "Episode: 194 | Intrinsic reward: 0.37199667096138\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 194 | Reward: 85.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0205\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 195 | Reward: 91.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0431\n",
      "\n",
      "Episode: 196 | Intrinsic reward: 0.37036725878715515\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 196 | Reward: 132.08  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0223\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 197 | Reward: 102.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0423\n",
      "\n",
      "Episode: 198 | Intrinsic reward: 0.3752511143684387\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 198 | Reward: 105.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0409\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 199 | Reward: 91.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0405\n",
      "\n",
      "Episode: 200 | Intrinsic reward: 0.3680762052536011\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 200 | Reward: 90.05  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0298\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 201 | Reward: 90.65  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0270\n",
      "\n",
      "Episode: 202 | Intrinsic reward: 0.36780428886413574\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 202 | Reward: 97.14  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0300\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 203 | Reward: 90.39  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0347\n",
      "\n",
      "Episode: 204 | Intrinsic reward: 0.3781718313694\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 204 | Reward: 123.21  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0271\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 205 | Reward: 125.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0371\n",
      "\n",
      "Episode: 206 | Intrinsic reward: 0.36341461539268494\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 206 | Reward: 153.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0274\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 207 | Reward: 126.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0286\n",
      "\n",
      "Episode: 208 | Intrinsic reward: 0.3695943355560303\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 208 | Reward: 121.02  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0208\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 209 | Reward: 110.77  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0181\n",
      "\n",
      "Episode: 210 | Intrinsic reward: 0.3703327775001526\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 210 | Reward: 98.48  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0252\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 211 | Reward: 96.99  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0228\n",
      "\n",
      "Episode: 212 | Intrinsic reward: 0.36795127391815186\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 212 | Reward: 91.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0225\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 213 | Reward: 95.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0346\n",
      "\n",
      "Episode: 214 | Intrinsic reward: 0.3711397647857666\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 214 | Reward: 107.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 215 | Reward: 88.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0433\n",
      "\n",
      "Episode: 216 | Intrinsic reward: 0.37058180570602417\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 216 | Reward: 84.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0405\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 217 | Reward: 97.13  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0323\n",
      "\n",
      "Episode: 218 | Intrinsic reward: 0.37328213453292847\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 218 | Reward: 91.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0388\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 219 | Reward: 108.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0418\n",
      "\n",
      "Episode: 220 | Intrinsic reward: 0.37007784843444824\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 220 | Reward: 153.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0323\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 221 | Reward: 96.00  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0210\n",
      "\n",
      "Episode: 222 | Intrinsic reward: 0.3737277388572693\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 222 | Reward: 114.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0317\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 223 | Reward: 130.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0277\n",
      "\n",
      "Episode: 224 | Intrinsic reward: 0.3711104989051819\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 224 | Reward: 94.36  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0492\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 225 | Reward: 97.17  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0256\n",
      "\n",
      "Episode: 226 | Intrinsic reward: 0.3708733320236206\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 226 | Reward: 102.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0304\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 227 | Reward: 123.13  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0233\n",
      "\n",
      "Episode: 228 | Intrinsic reward: 0.3634142279624939\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 228 | Reward: 115.30  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0361\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 229 | Reward: 125.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0369\n",
      "\n",
      "Episode: 230 | Intrinsic reward: 0.36822912096977234\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 230 | Reward: 107.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0375\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 231 | Reward: 90.43  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0147\n",
      "\n",
      "Episode: 232 | Intrinsic reward: 0.3541087210178375\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 232 | Reward: 102.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0391\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 233 | Reward: 132.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0334\n",
      "\n",
      "Episode: 234 | Intrinsic reward: 0.3700661063194275\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 234 | Reward: 89.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0259\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 235 | Reward: 85.35  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0355\n",
      "\n",
      "Episode: 236 | Intrinsic reward: 0.38209182024002075\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 236 | Reward: 168.96  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0339\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 237 | Reward: 141.36  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0453\n",
      "\n",
      "Episode: 238 | Intrinsic reward: 0.3698185086250305\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 238 | Reward: 90.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0393\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 239 | Reward: 154.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0227\n",
      "\n",
      "Episode: 240 | Intrinsic reward: 0.3627966046333313\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 240 | Reward: 117.94  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0474\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 241 | Reward: 91.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0383\n",
      "\n",
      "Episode: 242 | Intrinsic reward: 0.3714373707771301\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 242 | Reward: 119.67  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0397\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 243 | Reward: 91.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0459\n",
      "\n",
      "Episode: 244 | Intrinsic reward: 0.3698369264602661\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 244 | Reward: 155.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0363\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 245 | Reward: 126.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0200\n",
      "\n",
      "Episode: 246 | Intrinsic reward: 0.37230241298675537\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 246 | Reward: 95.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0251\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 247 | Reward: 91.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0301\n",
      "\n",
      "Episode: 248 | Intrinsic reward: 0.366294264793396\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 248 | Reward: 129.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0377\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 249 | Reward: 91.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0432\n",
      "\n",
      "Episode: 250 | Intrinsic reward: 0.3625500202178955\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 250 | Reward: 115.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 251 | Reward: 109.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0315\n",
      "\n",
      "Episode: 252 | Intrinsic reward: 0.3673297166824341\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 252 | Reward: 89.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0465\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 253 | Reward: 104.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      "Episode: 254 | Intrinsic reward: 0.36751049757003784\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 254 | Reward: 128.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0231\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 255 | Reward: 153.15  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0283\n",
      "\n",
      "Episode: 256 | Intrinsic reward: 0.36946970224380493\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 256 | Reward: 112.88  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0378\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 257 | Reward: 144.88  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0233\n",
      "\n",
      "Episode: 258 | Intrinsic reward: 0.3701210618019104\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 258 | Reward: 113.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0388\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 259 | Reward: 90.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0354\n",
      "\n",
      "Episode: 260 | Intrinsic reward: 0.37981393933296204\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 260 | Reward: 151.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0323\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 261 | Reward: 83.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0524\n",
      "\n",
      "Episode: 262 | Intrinsic reward: 0.36311405897140503\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 262 | Reward: 157.37  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0278\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 263 | Reward: 135.58  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0503\n",
      "\n",
      "Episode: 264 | Intrinsic reward: 0.37159717082977295\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 264 | Reward: 109.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0586\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 265 | Reward: 112.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0248\n",
      "\n",
      "Episode: 266 | Intrinsic reward: 0.36688870191574097\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 266 | Reward: 160.13  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0415\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 267 | Reward: 98.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0440\n",
      "\n",
      "Episode: 268 | Intrinsic reward: 0.36582836508750916\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 268 | Reward: 96.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0402\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 269 | Reward: 84.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0275\n",
      "\n",
      "Episode: 270 | Intrinsic reward: 0.37476807832717896\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 270 | Reward: 97.68  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0364\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 271 | Reward: 90.10  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0249\n",
      "\n",
      "Episode: 272 | Intrinsic reward: 0.37459442019462585\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 272 | Reward: 109.56  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0346\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 273 | Reward: 95.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0467\n",
      "\n",
      "Episode: 274 | Intrinsic reward: 0.36259084939956665\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 274 | Reward: 90.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0340\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 275 | Reward: 143.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0505\n",
      "\n",
      "Episode: 276 | Intrinsic reward: 0.37442952394485474\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 276 | Reward: 124.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0267\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 277 | Reward: 85.06  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0381\n",
      "\n",
      "Episode: 278 | Intrinsic reward: 0.3645535707473755\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 278 | Reward: 138.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0239\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 279 | Reward: 90.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0271\n",
      "\n",
      "Episode: 280 | Intrinsic reward: 0.368597149848938\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 280 | Reward: 104.45  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0294\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 281 | Reward: 97.21  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0326\n",
      "\n",
      "Episode: 282 | Intrinsic reward: 0.3722422122955322\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 282 | Reward: 85.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0317\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 283 | Reward: 126.42  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0291\n",
      "\n",
      "Episode: 284 | Intrinsic reward: 0.37185099720954895\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 284 | Reward: 88.17  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0274\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 285 | Reward: 95.21  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0525\n",
      "\n",
      "Episode: 286 | Intrinsic reward: 0.3716723322868347\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 286 | Reward: 90.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0352\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 287 | Reward: 96.25  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0287\n",
      "\n",
      "Episode: 288 | Intrinsic reward: 0.3686520457267761\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 288 | Reward: 91.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0262\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 289 | Reward: 120.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0334\n",
      "\n",
      "Episode: 290 | Intrinsic reward: 0.3730337917804718\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 290 | Reward: 89.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0370\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 291 | Reward: 113.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0321\n",
      "\n",
      "Episode: 292 | Intrinsic reward: 0.374187171459198\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 292 | Reward: 102.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0216\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 293 | Reward: 85.02  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0242\n",
      "\n",
      "Episode: 294 | Intrinsic reward: 0.36369428038597107\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 294 | Reward: 91.76  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0238\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 295 | Reward: 126.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0295\n",
      "\n",
      "Episode: 296 | Intrinsic reward: 0.3711183965206146\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 296 | Reward: 97.25  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0359\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 297 | Reward: 116.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0363\n",
      "\n",
      "Episode: 298 | Intrinsic reward: 0.36669135093688965\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 298 | Reward: 154.27  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0307\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 299 | Reward: 90.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0372\n",
      "\n",
      "Episode: 300 | Intrinsic reward: 0.3758660554885864\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 300 | Reward: 103.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0459\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 301 | Reward: 108.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0262\n",
      "\n",
      "Episode: 302 | Intrinsic reward: 0.36277127265930176\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 302 | Reward: 97.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0337\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 303 | Reward: 89.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0324\n",
      "\n",
      "Episode: 304 | Intrinsic reward: 0.36752891540527344\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 304 | Reward: 91.20  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 305 | Reward: 89.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0283\n",
      "\n",
      "Episode: 306 | Intrinsic reward: 0.37045466899871826\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 306 | Reward: 102.23  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0246\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 307 | Reward: 106.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0332\n",
      "\n",
      "Episode: 308 | Intrinsic reward: 0.35762298107147217\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 308 | Reward: 102.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0409\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 309 | Reward: 91.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0356\n",
      "\n",
      "Episode: 310 | Intrinsic reward: 0.36663174629211426\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 310 | Reward: 91.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0333\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 311 | Reward: 91.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0335\n",
      "\n",
      "Episode: 312 | Intrinsic reward: 0.3742298483848572\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 312 | Reward: 96.90  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0190\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 313 | Reward: 97.39  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0241\n",
      "\n",
      "Episode: 314 | Intrinsic reward: 0.3734084665775299\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 314 | Reward: 97.01  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0277\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 315 | Reward: 153.37  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0369\n",
      "\n",
      "Episode: 316 | Intrinsic reward: 0.3714291453361511\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 316 | Reward: 89.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0348\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 317 | Reward: 123.78  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0375\n",
      "\n",
      "Episode: 318 | Intrinsic reward: 0.3684834837913513\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 318 | Reward: 85.45  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0204\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 319 | Reward: 133.71  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 320 | Intrinsic reward: 0.3667604923248291\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 320 | Reward: 112.12  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0258\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 321 | Reward: 147.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0370\n",
      "\n",
      "Episode: 322 | Intrinsic reward: 0.36768150329589844\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 322 | Reward: 126.90  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 323 | Reward: 89.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0203\n",
      "\n",
      "Episode: 324 | Intrinsic reward: 0.3685190677642822\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 324 | Reward: 88.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0306\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 325 | Reward: 132.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0280\n",
      "\n",
      "Episode: 326 | Intrinsic reward: 0.36954760551452637\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 326 | Reward: 103.53  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0332\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 327 | Reward: 141.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0232\n",
      "\n",
      "Episode: 328 | Intrinsic reward: 0.36877331137657166\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 328 | Reward: 160.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0314\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 329 | Reward: 130.76  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0364\n",
      "\n",
      "Episode: 330 | Intrinsic reward: 0.3693540692329407\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 330 | Reward: 101.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0214\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 331 | Reward: 91.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0233\n",
      "\n",
      "Episode: 332 | Intrinsic reward: 0.37709879875183105\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 332 | Reward: 95.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0309\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 333 | Reward: 164.48  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0364\n",
      "\n",
      "Episode: 334 | Intrinsic reward: 0.36938244104385376\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 334 | Reward: 109.05  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0333\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 335 | Reward: 109.67  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0106\n",
      "\n",
      "Episode: 336 | Intrinsic reward: 0.3676111698150635\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 336 | Reward: 84.92  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0449\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 337 | Reward: 105.74  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0292\n",
      "\n",
      "Episode: 338 | Intrinsic reward: 0.3779886066913605\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 338 | Reward: 192.76  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0294\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 339 | Reward: 124.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0347\n",
      "\n",
      "Episode: 340 | Intrinsic reward: 0.3692514896392822\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 340 | Reward: 84.99  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0297\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 341 | Reward: 89.31  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0430\n",
      "\n",
      "Episode: 342 | Intrinsic reward: 0.37179034948349\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 342 | Reward: 122.15  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0336\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 343 | Reward: 90.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0272\n",
      "\n",
      "Episode: 344 | Intrinsic reward: 0.3708345890045166\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 344 | Reward: 89.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0373\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 345 | Reward: 161.37  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0358\n",
      "\n",
      "Episode: 346 | Intrinsic reward: 0.36774396896362305\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 346 | Reward: 113.27  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0362\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 347 | Reward: 129.96  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0391\n",
      "\n",
      "Episode: 348 | Intrinsic reward: 0.36865583062171936\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 348 | Reward: 132.72  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0428\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 349 | Reward: 96.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0257\n",
      "\n",
      "Episode: 350 | Intrinsic reward: 0.3707367777824402\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 350 | Reward: 123.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0258\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 351 | Reward: 122.53  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0309\n",
      "\n",
      "Episode: 352 | Intrinsic reward: 0.3672109544277191\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 352 | Reward: 128.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0340\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 353 | Reward: 91.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0335\n",
      "\n",
      "Episode: 354 | Intrinsic reward: 0.37739235162734985\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 354 | Reward: 91.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0285\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 355 | Reward: 114.00  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0355\n",
      "\n",
      "Episode: 356 | Intrinsic reward: 0.37158989906311035\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 356 | Reward: 123.46  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0312\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 357 | Reward: 97.91  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0265\n",
      "\n",
      "Episode: 358 | Intrinsic reward: 0.3739747405052185\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 358 | Reward: 127.96  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0309\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 359 | Reward: 90.73  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0257\n",
      "\n",
      "Episode: 360 | Intrinsic reward: 0.36324837803840637\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 360 | Reward: 84.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0344\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 361 | Reward: 91.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0327\n",
      "\n",
      "Episode: 362 | Intrinsic reward: 0.37361690402030945\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 362 | Reward: 96.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0403\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 363 | Reward: 100.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0395\n",
      "\n",
      "Episode: 364 | Intrinsic reward: 0.37354394793510437\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 364 | Reward: 90.65  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0202\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 365 | Reward: 111.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0352\n",
      "\n",
      "Episode: 366 | Intrinsic reward: 0.3685522675514221\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 366 | Reward: 113.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0266\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 367 | Reward: 120.08  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0225\n",
      "\n",
      "Episode: 368 | Intrinsic reward: 0.3754016160964966\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 368 | Reward: 91.65  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0299\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 369 | Reward: 90.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0409\n",
      "\n",
      "Episode: 370 | Intrinsic reward: 0.3746896982192993\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 370 | Reward: 90.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0288\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 371 | Reward: 115.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0322\n",
      "\n",
      "Episode: 372 | Intrinsic reward: 0.37726324796676636\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 372 | Reward: 139.94  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0415\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 373 | Reward: 91.73  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0363\n",
      "\n",
      "Episode: 374 | Intrinsic reward: 0.3665217161178589\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 374 | Reward: 96.23  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0339\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 375 | Reward: 152.84  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0330\n",
      "\n",
      "Episode: 376 | Intrinsic reward: 0.37246495485305786\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 376 | Reward: 97.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0399\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 377 | Reward: 116.98  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0353\n",
      "\n",
      "Episode: 378 | Intrinsic reward: 0.36832261085510254\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 378 | Reward: 142.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0219\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 379 | Reward: 96.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0268\n",
      "\n",
      "Episode: 380 | Intrinsic reward: 0.3678179085254669\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 380 | Reward: 95.34  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0290\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 381 | Reward: 108.79  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0345\n",
      "\n",
      "Episode: 382 | Intrinsic reward: 0.36978447437286377\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 382 | Reward: 97.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0331\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 383 | Reward: 127.79  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0434\n",
      "\n",
      "Episode: 384 | Intrinsic reward: 0.363974928855896\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 384 | Reward: 101.14  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0329\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 385 | Reward: 145.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0427\n",
      "\n",
      "Episode: 386 | Intrinsic reward: 0.3774494528770447\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 386 | Reward: 91.34  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0254\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 387 | Reward: 96.35  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0245\n",
      "\n",
      "Episode: 388 | Intrinsic reward: 0.373760461807251\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 388 | Reward: 106.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0265\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 389 | Reward: 84.20  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0311\n",
      "\n",
      "Episode: 390 | Intrinsic reward: 0.3669303059577942\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 390 | Reward: 99.91  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0303\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 391 | Reward: 97.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0318\n",
      "\n",
      "Episode: 392 | Intrinsic reward: 0.35861122608184814\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 392 | Reward: 136.73  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0379\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 393 | Reward: 91.43  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0318\n",
      "\n",
      "Episode: 394 | Intrinsic reward: 0.3723626732826233\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 394 | Reward: 91.65  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0222\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 395 | Reward: 91.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0177\n",
      "\n",
      "Episode: 396 | Intrinsic reward: 0.3653939962387085\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 396 | Reward: 97.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0198\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 397 | Reward: 126.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0299\n",
      "\n",
      "Episode: 398 | Intrinsic reward: 0.35814911127090454\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 398 | Reward: 119.84  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0211\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 399 | Reward: 97.48  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0366\n",
      "\n",
      "Episode: 400 | Intrinsic reward: 0.36413729190826416\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 400 | Reward: 90.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0409\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 401 | Reward: 90.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0370\n",
      "\n",
      "Episode: 402 | Intrinsic reward: 0.3711492419242859\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 402 | Reward: 89.47  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0393\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 403 | Reward: 103.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0354\n",
      "\n",
      "Episode: 404 | Intrinsic reward: 0.37551817297935486\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 404 | Reward: 130.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0439\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 405 | Reward: 127.82  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0291\n",
      "\n",
      "Episode: 406 | Intrinsic reward: 0.37198975682258606\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 406 | Reward: 91.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0441\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 407 | Reward: 90.27  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0437\n",
      "\n",
      "Episode: 408 | Intrinsic reward: 0.37475183606147766\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 408 | Reward: 91.03  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0359\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 409 | Reward: 119.31  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0275\n",
      "\n",
      "Episode: 410 | Intrinsic reward: 0.3775227665901184\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 410 | Reward: 190.58  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0568\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 411 | Reward: 143.36  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0213\n",
      "\n",
      "Episode: 412 | Intrinsic reward: 0.3702448904514313\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 412 | Reward: 96.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0370\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 413 | Reward: 89.01  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0222\n",
      "\n",
      "Episode: 414 | Intrinsic reward: 0.3654927611351013\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 414 | Reward: 102.24  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0516\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 415 | Reward: 90.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0555\n",
      "\n",
      "Episode: 416 | Intrinsic reward: 0.3828918933868408\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 416 | Reward: 97.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0372\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 417 | Reward: 89.73  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0413\n",
      "\n",
      "Episode: 418 | Intrinsic reward: 0.36086639761924744\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 418 | Reward: 120.12  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0228\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 419 | Reward: 91.06  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0315\n",
      "\n",
      "Episode: 420 | Intrinsic reward: 0.3701271414756775\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 420 | Reward: 140.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0336\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 421 | Reward: 106.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0447\n",
      "\n",
      "Episode: 422 | Intrinsic reward: 0.35575729608535767\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 422 | Reward: 90.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0237\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 423 | Reward: 110.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0239\n",
      "\n",
      "Episode: 424 | Intrinsic reward: 0.36021187901496887\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 424 | Reward: 89.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0297\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 425 | Reward: 115.36  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0335\n",
      "\n",
      "Episode: 426 | Intrinsic reward: 0.37314343452453613\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 426 | Reward: 86.01  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0329\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 427 | Reward: 96.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0508\n",
      "\n",
      "Episode: 428 | Intrinsic reward: 0.3800898790359497\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 428 | Reward: 125.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0303\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 429 | Reward: 97.95  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0217\n",
      "\n",
      "Episode: 430 | Intrinsic reward: 0.3677927851676941\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 430 | Reward: 101.85  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0391\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 431 | Reward: 163.17  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0284\n",
      "\n",
      "Episode: 432 | Intrinsic reward: 0.37210536003112793\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 432 | Reward: 90.19  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0383\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 433 | Reward: 90.09  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0287\n",
      "\n",
      "Episode: 434 | Intrinsic reward: 0.3759242296218872\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 434 | Reward: 91.40  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0465\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 435 | Reward: 90.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0188\n",
      "\n",
      "Episode: 436 | Intrinsic reward: 0.36973556876182556\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 436 | Reward: 91.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0322\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 437 | Reward: 106.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0270\n",
      "\n",
      "Episode: 438 | Intrinsic reward: 0.38010621070861816\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 438 | Reward: 85.58  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0337\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 439 | Reward: 145.30  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0325\n",
      "\n",
      "Episode: 440 | Intrinsic reward: 0.37303978204727173\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 440 | Reward: 90.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0252\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 441 | Reward: 133.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0244\n",
      "\n",
      "Episode: 442 | Intrinsic reward: 0.3705558776855469\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 442 | Reward: 91.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0342\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 443 | Reward: 116.50  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0239\n",
      "\n",
      "Episode: 444 | Intrinsic reward: 0.3631208539009094\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 444 | Reward: 109.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0389\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 445 | Reward: 90.39  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0165\n",
      "\n",
      "Episode: 446 | Intrinsic reward: 0.36412495374679565\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 446 | Reward: 91.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0348\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 447 | Reward: 91.26  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0138\n",
      "\n",
      "Episode: 448 | Intrinsic reward: 0.37073948979377747\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 448 | Reward: 101.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0268\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 449 | Reward: 111.47  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0302\n",
      "\n",
      "Episode: 450 | Intrinsic reward: 0.3776640295982361\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 450 | Reward: 91.64  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0448\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 451 | Reward: 91.56  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0304\n",
      "\n",
      "Episode: 452 | Intrinsic reward: 0.36100220680236816\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 452 | Reward: 97.32  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0402\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 453 | Reward: 108.34  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0384\n",
      "\n",
      "Episode: 454 | Intrinsic reward: 0.3778337240219116\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 454 | Reward: 91.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0303\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 455 | Reward: 90.44  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0409\n",
      "\n",
      "Episode: 456 | Intrinsic reward: 0.36852121353149414\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 456 | Reward: 90.78  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0260\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 457 | Reward: 91.16  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0330\n",
      "\n",
      "Episode: 458 | Intrinsic reward: 0.37163493037223816\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 458 | Reward: 103.84  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0210\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 459 | Reward: 97.34  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0351\n",
      "\n",
      "Episode: 460 | Intrinsic reward: 0.3761798143386841\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 460 | Reward: 108.50  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0316\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 461 | Reward: 146.07  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0242\n",
      "\n",
      "Episode: 462 | Intrinsic reward: 0.37258175015449524\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 462 | Reward: 111.06  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0366\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 463 | Reward: 88.84  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0365\n",
      "\n",
      "Episode: 464 | Intrinsic reward: 0.3721659779548645\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 464 | Reward: 248.10  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0379\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 465 | Reward: 84.69  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0374\n",
      "\n",
      "Episode: 466 | Intrinsic reward: 0.3717159628868103\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 466 | Reward: 91.17  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0307\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 467 | Reward: 91.06  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0285\n",
      "\n",
      "Episode: 468 | Intrinsic reward: 0.3789900541305542\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 468 | Reward: 91.50  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0439\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 469 | Reward: 145.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0317\n",
      "\n",
      "Episode: 470 | Intrinsic reward: 0.3708478808403015\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 470 | Reward: 90.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0416\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 471 | Reward: 90.88  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0318\n",
      "\n",
      "Episode: 472 | Intrinsic reward: 0.3690892457962036\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 472 | Reward: 90.83  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0289\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 473 | Reward: 90.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0292\n",
      "\n",
      "Episode: 474 | Intrinsic reward: 0.3699573874473572\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 474 | Reward: 96.18  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0490\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 475 | Reward: 96.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0334\n",
      "\n",
      "Episode: 476 | Intrinsic reward: 0.3628067970275879\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 476 | Reward: 85.89  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0442\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 477 | Reward: 96.66  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0320\n",
      "\n",
      "Episode: 478 | Intrinsic reward: 0.3615771532058716\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 478 | Reward: 97.59  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0410\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 479 | Reward: 91.28  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0308\n",
      "\n",
      "Episode: 480 | Intrinsic reward: 0.3583942949771881\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 480 | Reward: 103.31  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0434\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 481 | Reward: 152.60  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0339\n",
      "\n",
      "Episode: 482 | Intrinsic reward: 0.3656447231769562\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 482 | Reward: 90.96  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0399\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 483 | Reward: 92.76  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0288\n",
      "\n",
      "Episode: 484 | Intrinsic reward: 0.37420469522476196\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 484 | Reward: 91.33  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0332\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 485 | Reward: 90.52  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0263\n",
      "\n",
      "Episode: 486 | Intrinsic reward: 0.37251198291778564\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 486 | Reward: 134.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0296\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 487 | Reward: 88.87  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0299\n",
      "\n",
      "Episode: 488 | Intrinsic reward: 0.3682831823825836\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 488 | Reward: 187.05  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0220\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 489 | Reward: 96.11  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0297\n",
      "\n",
      "Episode: 490 | Intrinsic reward: 0.37040555477142334\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 490 | Reward: 91.36  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0261\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 491 | Reward: 96.25  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0292\n",
      "\n",
      "Episode: 492 | Intrinsic reward: 0.37485143542289734\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 492 | Reward: 91.37  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0374\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 493 | Reward: 131.00  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0306\n",
      "\n",
      "Episode: 494 | Intrinsic reward: 0.36932897567749023\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 494 | Reward: 91.41  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0345\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 495 | Reward: 104.42  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0490\n",
      "\n",
      "Episode: 496 | Intrinsic reward: 0.3674290180206299\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 496 | Reward: 159.54  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0176\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 497 | Reward: 104.70  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0397\n",
      "\n",
      "Episode: 498 | Intrinsic reward: 0.37109237909317017\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 498 | Reward: 96.86  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0406\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 499 | Reward: 103.53  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0516\n",
      "\n",
      "Episode: 500 | Intrinsic reward: 0.36230090260505676\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 500 | Reward: 96.76  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0349\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 501 | Reward: 105.00  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0192\n",
      "\n",
      "Episode: 502 | Intrinsic reward: 0.3744050860404968\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 502 | Reward: 149.61  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0442\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 503 | Reward: 110.29  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0307\n",
      "\n",
      "Episode: 504 | Intrinsic reward: 0.37442195415496826\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 504 | Reward: 89.30  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0497\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 505 | Reward: 90.55  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0437\n",
      "\n",
      "Episode: 506 | Intrinsic reward: 0.3581007421016693\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 506 | Reward: 97.13  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0326\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 507 | Reward: 153.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0441\n",
      "\n",
      "Episode: 508 | Intrinsic reward: 0.37303924560546875\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 508 | Reward: 140.04  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0231\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 509 | Reward: 103.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0238\n",
      "\n",
      "Episode: 510 | Intrinsic reward: 0.3655890226364136\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 510 | Reward: 124.93  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0364\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 511 | Reward: 203.22  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0444\n",
      "\n",
      "Episode: 512 | Intrinsic reward: 0.37019699811935425\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 512 | Reward: 135.51  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0394\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 513 | Reward: 133.48  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0363\n",
      "\n",
      "Episode: 514 | Intrinsic reward: 0.35722047090530396\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 514 | Reward: 130.38  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0240\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 515 | Reward: 96.94  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0240\n",
      "\n",
      "Episode: 516 | Intrinsic reward: 0.3698463439941406\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 516 | Reward: 94.97  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0367\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 517 | Reward: 129.57  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0398\n",
      "\n",
      "Episode: 518 | Intrinsic reward: 0.37485820055007935\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 518 | Reward: 96.49  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0263\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 519 | Reward: 162.63  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0388\n",
      "\n",
      "Episode: 520 | Intrinsic reward: 0.3763825595378876\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 520 | Reward: 88.75  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0240\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 521 | Reward: 156.74  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0354\n",
      "\n",
      "Episode: 522 | Intrinsic reward: 0.37588903307914734\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 522 | Reward: 91.08  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0431\n",
      "\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 523 | Reward: 96.94  Actor Loss: 0.0000 | Critic Loss: 0.0000 | Alpha: 0.0000\n",
      "World Loss: 0.0419\n",
      "\n",
      "Episode: 524 | Intrinsic reward: 0.36084088683128357\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 524 | Reward: 150.82  Actor Loss: 4.1212 | Critic Loss: 2.5041 | Alpha: 0.3000\n",
      "World Loss: 0.0227\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 525 | Reward: 97.06  Actor Loss: 4.0958 | Critic Loss: 2.4463 | Alpha: 0.2999\n",
      "World Loss: 0.0365\n",
      "\n",
      "Episode: 526 | Intrinsic reward: 0.3692476749420166\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 526 | Reward: 102.59  Actor Loss: 4.0462 | Critic Loss: 2.6443 | Alpha: 0.2999\n",
      "World Loss: 0.0299\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 527 | Reward: 120.46  Actor Loss: 4.0550 | Critic Loss: 2.4902 | Alpha: 0.2999\n",
      "World Loss: 0.0244\n",
      "\n",
      "Episode: 528 | Intrinsic reward: 0.36851274967193604\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 528 | Reward: 159.64  Actor Loss: 4.0715 | Critic Loss: 2.4959 | Alpha: 0.2999\n",
      "World Loss: 0.0262\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 529 | Reward: 102.93  Actor Loss: 4.0608 | Critic Loss: 2.5422 | Alpha: 0.2998\n",
      "World Loss: 0.0313\n",
      "\n",
      "Episode: 530 | Intrinsic reward: 0.36179912090301514\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 530 | Reward: 97.91  Actor Loss: 4.1207 | Critic Loss: 2.5626 | Alpha: 0.2998\n",
      "World Loss: 0.0427\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 531 | Reward: 114.05  Actor Loss: 4.0956 | Critic Loss: 2.4750 | Alpha: 0.2998\n",
      "World Loss: 0.0206\n",
      "\n",
      "Episode: 532 | Intrinsic reward: 0.3682614862918854\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 532 | Reward: 103.20  Actor Loss: 4.1170 | Critic Loss: 2.5203 | Alpha: 0.2997\n",
      "World Loss: 0.0396\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 533 | Reward: 90.92  Actor Loss: 4.0663 | Critic Loss: 2.4597 | Alpha: 0.2997\n",
      "World Loss: 0.0254\n",
      "\n",
      "Episode: 534 | Intrinsic reward: 0.37208348512649536\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 534 | Reward: 115.43  Actor Loss: 4.0870 | Critic Loss: 2.5963 | Alpha: 0.2997\n",
      "World Loss: 0.0449\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 535 | Reward: 103.59  Actor Loss: 4.0669 | Critic Loss: 2.4281 | Alpha: 0.2996\n",
      "World Loss: 0.0308\n",
      "\n",
      "Episode: 536 | Intrinsic reward: 0.37847447395324707\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 536 | Reward: 97.62  Actor Loss: 4.0617 | Critic Loss: 2.4778 | Alpha: 0.2996\n",
      "World Loss: 0.0218\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 537 | Reward: 157.70  Actor Loss: 4.0684 | Critic Loss: 2.6027 | Alpha: 0.2996\n",
      "World Loss: 0.0278\n",
      "\n",
      "Episode: 538 | Intrinsic reward: 0.36752140522003174\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 538 | Reward: 128.17  Actor Loss: 4.0523 | Critic Loss: 2.4656 | Alpha: 0.2996\n",
      "World Loss: 0.0331\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 539 | Reward: 202.15  Actor Loss: 4.0575 | Critic Loss: 2.4918 | Alpha: 0.2995\n",
      "World Loss: 0.0332\n",
      "\n",
      "Episode: 540 | Intrinsic reward: 0.3749409019947052\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 540 | Reward: 90.63  Actor Loss: 4.0468 | Critic Loss: 2.5335 | Alpha: 0.2995\n",
      "World Loss: 0.0419\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 541 | Reward: 90.78  Actor Loss: 4.0900 | Critic Loss: 2.4963 | Alpha: 0.2995\n",
      "World Loss: 0.0301\n",
      "\n",
      "Episode: 542 | Intrinsic reward: 0.3738749921321869\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 542 | Reward: 96.81  Actor Loss: 4.0951 | Critic Loss: 2.4496 | Alpha: 0.2994\n",
      "World Loss: 0.0185\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 543 | Reward: 96.06  Actor Loss: 4.1467 | Critic Loss: 2.4430 | Alpha: 0.2994\n",
      "World Loss: 0.0451\n",
      "\n",
      "Episode: 544 | Intrinsic reward: 0.3594852089881897\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 544 | Reward: 83.04  Actor Loss: 4.0674 | Critic Loss: 2.5591 | Alpha: 0.2994\n",
      "World Loss: 0.0397\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 545 | Reward: 91.34  Actor Loss: 4.0162 | Critic Loss: 2.3929 | Alpha: 0.2993\n",
      "World Loss: 0.0348\n",
      "\n",
      "Episode: 546 | Intrinsic reward: 0.36864447593688965\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 546 | Reward: 97.00  Actor Loss: 4.0455 | Critic Loss: 2.5486 | Alpha: 0.2993\n",
      "World Loss: 0.0306\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 547 | Reward: 91.26  Actor Loss: 4.0451 | Critic Loss: 2.6267 | Alpha: 0.2993\n",
      "World Loss: 0.0322\n",
      "\n",
      "Episode: 548 | Intrinsic reward: 0.36875176429748535\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 548 | Reward: 102.68  Actor Loss: 4.0902 | Critic Loss: 2.4539 | Alpha: 0.2993\n",
      "World Loss: 0.0240\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 549 | Reward: 113.23  Actor Loss: 4.0390 | Critic Loss: 2.4692 | Alpha: 0.2992\n",
      "World Loss: 0.0319\n",
      "\n",
      "Episode: 550 | Intrinsic reward: 0.36534827947616577\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 550 | Reward: 90.79  Actor Loss: 4.0481 | Critic Loss: 2.4355 | Alpha: 0.2992\n",
      "World Loss: 0.0534\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 551 | Reward: 91.13  Actor Loss: 4.0851 | Critic Loss: 2.4665 | Alpha: 0.2992\n",
      "World Loss: 0.0274\n",
      "\n",
      "Episode: 552 | Intrinsic reward: 0.3683978021144867\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 552 | Reward: 96.75  Actor Loss: 3.9906 | Critic Loss: 2.4969 | Alpha: 0.2991\n",
      "World Loss: 0.0372\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 553 | Reward: 122.30  Actor Loss: 4.0628 | Critic Loss: 2.5796 | Alpha: 0.2991\n",
      "World Loss: 0.0274\n",
      "\n",
      "Episode: 554 | Intrinsic reward: 0.3685755431652069\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 554 | Reward: 91.20  Actor Loss: 4.0901 | Critic Loss: 2.4816 | Alpha: 0.2991\n",
      "World Loss: 0.0400\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 555 | Reward: 165.97  Actor Loss: 4.0389 | Critic Loss: 2.4845 | Alpha: 0.2990\n",
      "World Loss: 0.0385\n",
      "\n",
      "Episode: 556 | Intrinsic reward: 0.3756778836250305\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 556 | Reward: 110.16  Actor Loss: 4.0577 | Critic Loss: 2.5859 | Alpha: 0.2990\n",
      "World Loss: 0.0227\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 557 | Reward: 96.94  Actor Loss: 4.0630 | Critic Loss: 2.5164 | Alpha: 0.2990\n",
      "World Loss: 0.0311\n",
      "\n",
      "Episode: 558 | Intrinsic reward: 0.3586299419403076\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 558 | Reward: 133.99  Actor Loss: 4.0596 | Critic Loss: 2.5360 | Alpha: 0.2990\n",
      "World Loss: 0.0357\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 559 | Reward: 176.16  Actor Loss: 4.0307 | Critic Loss: 2.5555 | Alpha: 0.2989\n",
      "World Loss: 0.0340\n",
      "\n",
      "Episode: 560 | Intrinsic reward: 0.36314699053764343\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 560 | Reward: 101.15  Actor Loss: 4.1101 | Critic Loss: 2.5147 | Alpha: 0.2989\n",
      "World Loss: 0.0237\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 561 | Reward: 90.00  Actor Loss: 4.0481 | Critic Loss: 2.4598 | Alpha: 0.2989\n",
      "World Loss: 0.0293\n",
      "\n",
      "Episode: 562 | Intrinsic reward: 0.3700562119483948\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 562 | Reward: 89.19  Actor Loss: 4.0349 | Critic Loss: 2.5650 | Alpha: 0.2988\n",
      "World Loss: 0.0279\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 563 | Reward: 134.69  Actor Loss: 4.0531 | Critic Loss: 2.6768 | Alpha: 0.2988\n",
      "World Loss: 0.0276\n",
      "\n",
      "Episode: 564 | Intrinsic reward: 0.369518518447876\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 564 | Reward: 102.98  Actor Loss: 4.0847 | Critic Loss: 2.4512 | Alpha: 0.2988\n",
      "World Loss: 0.0481\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 565 | Reward: 90.81  Actor Loss: 4.0288 | Critic Loss: 2.5163 | Alpha: 0.2987\n",
      "World Loss: 0.0385\n",
      "\n",
      "Episode: 566 | Intrinsic reward: 0.3784569501876831\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 566 | Reward: 96.25  Actor Loss: 3.9702 | Critic Loss: 2.5946 | Alpha: 0.2987\n",
      "World Loss: 0.0401\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 567 | Reward: 96.22  Actor Loss: 4.0509 | Critic Loss: 2.5784 | Alpha: 0.2987\n",
      "World Loss: 0.0273\n",
      "\n",
      "Episode: 568 | Intrinsic reward: 0.3763960599899292\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 568 | Reward: 91.09  Actor Loss: 4.1406 | Critic Loss: 2.5525 | Alpha: 0.2987\n",
      "World Loss: 0.0390\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 569 | Reward: 97.77  Actor Loss: 4.0649 | Critic Loss: 2.5335 | Alpha: 0.2986\n",
      "World Loss: 0.0342\n",
      "\n",
      "Episode: 570 | Intrinsic reward: 0.37646687030792236\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 570 | Reward: 89.86  Actor Loss: 4.0546 | Critic Loss: 2.5473 | Alpha: 0.2986\n",
      "World Loss: 0.0285\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 571 | Reward: 90.12  Actor Loss: 4.0497 | Critic Loss: 2.4618 | Alpha: 0.2986\n",
      "World Loss: 0.0359\n",
      "\n",
      "Episode: 572 | Intrinsic reward: 0.3588355779647827\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 572 | Reward: 91.67  Actor Loss: 4.0244 | Critic Loss: 2.5289 | Alpha: 0.2985\n",
      "World Loss: 0.0318\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 573 | Reward: 144.29  Actor Loss: 4.0808 | Critic Loss: 2.4835 | Alpha: 0.2985\n",
      "World Loss: 0.0232\n",
      "\n",
      "Episode: 574 | Intrinsic reward: 0.3656591773033142\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 574 | Reward: 90.91  Actor Loss: 4.0558 | Critic Loss: 2.4732 | Alpha: 0.2985\n",
      "World Loss: 0.0217\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 575 | Reward: 90.24  Actor Loss: 4.0326 | Critic Loss: 2.5968 | Alpha: 0.2984\n",
      "World Loss: 0.0436\n",
      "\n",
      "Episode: 576 | Intrinsic reward: 0.3698328137397766\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 576 | Reward: 102.67  Actor Loss: 4.1223 | Critic Loss: 2.4861 | Alpha: 0.2984\n",
      "World Loss: 0.0267\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 577 | Reward: 110.80  Actor Loss: 4.0207 | Critic Loss: 2.4693 | Alpha: 0.2984\n",
      "World Loss: 0.0272\n",
      "\n",
      "Episode: 578 | Intrinsic reward: 0.3734760284423828\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 578 | Reward: 90.94  Actor Loss: 4.0601 | Critic Loss: 2.4850 | Alpha: 0.2984\n",
      "World Loss: 0.0288\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 579 | Reward: 97.45  Actor Loss: 4.0362 | Critic Loss: 2.4637 | Alpha: 0.2983\n",
      "World Loss: 0.0264\n",
      "\n",
      "Episode: 580 | Intrinsic reward: 0.37085607647895813\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 580 | Reward: 91.30  Actor Loss: 4.0702 | Critic Loss: 2.5314 | Alpha: 0.2983\n",
      "World Loss: 0.0274\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 581 | Reward: 97.40  Actor Loss: 4.0059 | Critic Loss: 2.4238 | Alpha: 0.2983\n",
      "World Loss: 0.0250\n",
      "\n",
      "Episode: 582 | Intrinsic reward: 0.36920398473739624\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 582 | Reward: 104.58  Actor Loss: 4.0369 | Critic Loss: 2.6122 | Alpha: 0.2982\n",
      "World Loss: 0.0507\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 583 | Reward: 114.11  Actor Loss: 4.0039 | Critic Loss: 2.4666 | Alpha: 0.2982\n",
      "World Loss: 0.0459\n",
      "\n",
      "Episode: 584 | Intrinsic reward: 0.3768191635608673\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 584 | Reward: 91.61  Actor Loss: 4.0162 | Critic Loss: 2.6011 | Alpha: 0.2982\n",
      "World Loss: 0.0410\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 585 | Reward: 102.47  Actor Loss: 4.0956 | Critic Loss: 2.5277 | Alpha: 0.2981\n",
      "World Loss: 0.0374\n",
      "\n",
      "Episode: 586 | Intrinsic reward: 0.3736010491847992\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 586 | Reward: 109.14  Actor Loss: 4.0236 | Critic Loss: 2.4998 | Alpha: 0.2981\n",
      "World Loss: 0.0349\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 587 | Reward: 90.38  Actor Loss: 4.1116 | Critic Loss: 2.4211 | Alpha: 0.2981\n",
      "World Loss: 0.0244\n",
      "\n",
      "Episode: 588 | Intrinsic reward: 0.36809563636779785\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 588 | Reward: 91.15  Actor Loss: 4.0512 | Critic Loss: 2.5019 | Alpha: 0.2981\n",
      "World Loss: 0.0331\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 589 | Reward: 93.05  Actor Loss: 4.0367 | Critic Loss: 2.3808 | Alpha: 0.2980\n",
      "World Loss: 0.0311\n",
      "\n",
      "Episode: 590 | Intrinsic reward: 0.3728477358818054\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 590 | Reward: 90.51  Actor Loss: 4.0342 | Critic Loss: 2.5129 | Alpha: 0.2980\n",
      "World Loss: 0.0259\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 591 | Reward: 103.33  Actor Loss: 4.0323 | Critic Loss: 2.6022 | Alpha: 0.2980\n",
      "World Loss: 0.0348\n",
      "\n",
      "Episode: 592 | Intrinsic reward: 0.37243515253067017\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 592 | Reward: 103.14  Actor Loss: 4.0502 | Critic Loss: 2.5149 | Alpha: 0.2979\n",
      "World Loss: 0.0345\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 593 | Reward: 162.89  Actor Loss: 4.0407 | Critic Loss: 2.5154 | Alpha: 0.2979\n",
      "World Loss: 0.0390\n",
      "\n",
      "Episode: 594 | Intrinsic reward: 0.3782694935798645\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 594 | Reward: 102.01  Actor Loss: 4.0192 | Critic Loss: 2.6215 | Alpha: 0.2979\n",
      "World Loss: 0.0246\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 595 | Reward: 84.47  Actor Loss: 4.0159 | Critic Loss: 2.5428 | Alpha: 0.2979\n",
      "World Loss: 0.0287\n",
      "\n",
      "Episode: 596 | Intrinsic reward: 0.3699837327003479\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 596 | Reward: 142.72  Actor Loss: 4.0034 | Critic Loss: 2.6048 | Alpha: 0.2978\n",
      "World Loss: 0.0360\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 597 | Reward: 107.54  Actor Loss: 4.0137 | Critic Loss: 2.5844 | Alpha: 0.2978\n",
      "World Loss: 0.0309\n",
      "\n",
      "Episode: 598 | Intrinsic reward: 0.3700491786003113\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 598 | Reward: 104.35  Actor Loss: 4.0365 | Critic Loss: 2.5163 | Alpha: 0.2978\n",
      "World Loss: 0.0371\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 599 | Reward: 91.06  Actor Loss: 4.1047 | Critic Loss: 2.5065 | Alpha: 0.2977\n",
      "World Loss: 0.0497\n",
      "\n",
      "Episode: 600 | Intrinsic reward: 0.3594691753387451\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 600 | Reward: 90.75  Actor Loss: 4.0382 | Critic Loss: 2.4023 | Alpha: 0.2977\n",
      "World Loss: 0.0545\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 601 | Reward: 99.71  Actor Loss: 4.0332 | Critic Loss: 2.3576 | Alpha: 0.2977\n",
      "World Loss: 0.0231\n",
      "\n",
      "Episode: 602 | Intrinsic reward: 0.35972410440444946\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 602 | Reward: 90.88  Actor Loss: 4.0153 | Critic Loss: 2.5939 | Alpha: 0.2976\n",
      "World Loss: 0.0191\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 603 | Reward: 121.09  Actor Loss: 3.9817 | Critic Loss: 2.5285 | Alpha: 0.2976\n",
      "World Loss: 0.0259\n",
      "\n",
      "Episode: 604 | Intrinsic reward: 0.357649028301239\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 604 | Reward: 112.68  Actor Loss: 4.0507 | Critic Loss: 2.5211 | Alpha: 0.2976\n",
      "World Loss: 0.0330\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 605 | Reward: 145.70  Actor Loss: 3.9930 | Critic Loss: 2.5034 | Alpha: 0.2976\n",
      "World Loss: 0.0353\n",
      "\n",
      "Episode: 606 | Intrinsic reward: 0.364240825176239\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 606 | Reward: 91.20  Actor Loss: 4.0492 | Critic Loss: 2.5486 | Alpha: 0.2975\n",
      "World Loss: 0.0185\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 607 | Reward: 97.06  Actor Loss: 4.0224 | Critic Loss: 2.5783 | Alpha: 0.2975\n",
      "World Loss: 0.0364\n",
      "\n",
      "Episode: 608 | Intrinsic reward: 0.35853126645088196\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 608 | Reward: 142.37  Actor Loss: 4.0569 | Critic Loss: 2.7008 | Alpha: 0.2975\n",
      "World Loss: 0.0306\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 609 | Reward: 86.80  Actor Loss: 3.9637 | Critic Loss: 2.4168 | Alpha: 0.2974\n",
      "World Loss: 0.0345\n",
      "\n",
      "Episode: 610 | Intrinsic reward: 0.36295026540756226\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Episode: 610 | Reward: 88.51  Actor Loss: 3.9788 | Critic Loss: 2.5089 | Alpha: 0.2974\n",
      "World Loss: 0.0276\n",
      "\n",
      "i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 348])\n",
      "After Concat : i_ns: torch.Size([192, 256]) | ns: torch.Size([64, 256]) | Next states: torch.Size([256, 256])\n",
      "After s: torch.Size([64, 256]) | i_s: torch.Size([192, 256]) | states: torch.Size([256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[403], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msac_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msac_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmix_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld_optimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld_scheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroll_out_break\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[402], line 53\u001b[0m, in \u001b[0;36mtraining_block\u001b[1;34m(env, agent, world_model, world_loss_function, sac_buffer, mix_batch, max_episodes, world_optimizer, world_scheduler, encoder, batch_size, roll_out_break, memory, warm_up)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Update SAC with mixed buffer—only if imagination is ready\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mix_batch\u001b[38;5;241m.\u001b[39mimagination_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m---> 53\u001b[0m     actor_loss, critic_loss, alpha \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmix_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m#actor_loss, critic_loss, alpha = agent.update(sac_buffer, batch_size)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "Cell \u001b[1;32mIn[393], line 86\u001b[0m, in \u001b[0;36mSac_Agent.update\u001b[1;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# compute critic loss\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#latent_states = self.encode(states)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m new_action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(states)\n\u001b[1;32m---> 86\u001b[0m critic_1, critic_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m critic_loss_1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(critic_1, target_qvals)\n\u001b[0;32m     88\u001b[0m critic_loss_2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(critic_2, target_qvals)\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[391], line 49\u001b[0m, in \u001b[0;36mCritic_Network.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     45\u001b[0m cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Feature -> layer norm -> Unsqueeze\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(feature)\n\u001b[0;32m     52\u001b[0m norm_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(feature)\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[389], line 29\u001b[0m, in \u001b[0;36mFeature_extractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_block(\n",
    "    env,\n",
    "    sac_agent,\n",
    "    world_model,\n",
    "    world_loss_function,\n",
    "    sac_buffer,\n",
    "    mix_batch,\n",
    "    max_episodes = 3000,\n",
    "    world_optimizer = optimizer,\n",
    "    world_scheduler = scheduler,\n",
    "    encoder = encode,\n",
    "    batch_size = 256,\n",
    "    roll_out_break = 2,\n",
    "    memory = None,\n",
    "    warm_up = 10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
