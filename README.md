*ğŸŒŒ Plan2Explore: Chasing Rewards in Latent Realms*

â€œPg 66 of the Rosetta Stone of the mind says: We understand the context of experience, but fail to record it. Thus, relations fade before they form. In RL, too, we modeled dreams, but always kept them in realityâ€™s shape. Plan2Explore dares to go furtherâ€”it dreams in latent space.â€

**ğŸŒŸ Overview**

Plan2Explore is a curiosity-driven reinforcement learning (RL) algorithm that thrives in sparse-reward environments by learning world dynamics in latent space. Unlike traditional agents (like MBPO) which operate in observation space for both real and imagined experiences, Plan2Explore transitions to latent representationsâ€”enhancing generalization and exploration.

This project is not just a reinforcement learning algorithmâ€”itâ€™s a philosophical shift from chasing rewards to understanding the unknown. Inspired by human curiosity, Plan2Explore updates the agent through surprise and predictive disagreement in a learned world model.

**ğŸš€ Motivation**

Model-based RL has long aimed to reduce sample inefficiency, but often struggles with high-dimensional raw inputs and sparse feedback. While MBPO grounded its learning in real-world rollouts, Plan2Explore exploresâ€”literallyâ€”by imagining futures in a compact latent space and rewarding itself for being surprised.

**ğŸ§  Architecture**

Plan2Explore's design blends structure and surprise:

    Encoder â€“ Converts raw observations into latent representations.
    
    GRU World Model â€“ Predicts future latent states through recurrent dynamics.
    
    Ensemble Module â€“ A disagreement-driven module for intrinsic reward.
    
    World Loss â€“ Joint reconstruction and forward prediction losses for stable world learning.
    
    Feature Extractor â€“ Processes latent dynamics into meaningful features.
    
    Actor & Critic â€“ Policy and value functions operating in latent space.
    
    Target Networks â€“ For soft target updates and stability.
    
    Two Buffers â€“ Real and imagined transitions (sac_buffer + imagination_buffer).
    
    Mixed Batch Logic â€“ Balanced sampling from both buffers to update SAC.
    
    Intrinsic Reward Generator â€“ Based on ensemble disagreement.
    
    Training Loop â€“ Integrates SAC and world updates in a unified flow.

 **ğŸ§ª Key Techniques & Skills Used**

    âœ… Reinforcement Learning (Soft Actor-Critic)
    
    âœ… Recurrent Models (GRU-based Dynamics)
    
    âœ… Latent Space Modeling
    
    âœ… Curiosity-Driven Exploration (Intrinsic Motivation)
    
    âœ… Ensemble Modeling for Uncertainty
    
    âœ… World Models
    
    âœ… Mixed Buffer Sampling
    
    âœ… Advanced PyTorch
    
    âœ… Gradient Clipping, Scheduling, and Target Updates
    
    âœ… Shape mismatch Strategy

**ğŸ“ˆ Sample Logs**


    Episode: 1386 | Intrinsic Reward: 0.383
    Reward: 109.10  | Actor Loss: 3.70 | Critic Loss: 2.59 | Alpha: 0.2752
    World Loss: 0.0396
    
ğŸ› ï¸ Installation

git clone https://github.com/your_username/Plan2Explore.git
cd Plan2Explore
pip install -r requirements.txt

**ğŸ“¦ Important Code Snippets**

intrinsic_reward_function(![code](https://github.com/user-attachments/assets/2ada54a6-fed5-43f5-b35f-ed96bfbc9d82)
)

GRUWorldModel.forward(![code_1](https://github.com/user-attachments/assets/1990d22e-feed-4ce3-83fb-d0638245fddb)
)

MixBatch.sample(![code_2](https://github.com/user-attachments/assets/9407e948-9801-4cc9-9eab-460aefb40a1c)
)

Agent.update(![code_3](https://github.com/user-attachments/assets/21fc081a-cd15-4c7b-b120-8ec82ca79075)
)


**ğŸ”­ Future Directions**

**The next leap is a return to rootsâ€”a return to model-free RL with a deep understanding of latent dynamics. The goal: TRPO-based exploration that blends curiosity with structure, with zero reliance on learned world models.**


**â¤ï¸ Author's Note**

This project is part of a greater journey toward AI that dreams, wonders, and learns like us. Built during long nights of silence, Plan2Explore is a tribute to solitude, obsession, and the joy of learning from the unknown.

â€œI do not chase rewardsâ€”I chase the reason for them.â€
