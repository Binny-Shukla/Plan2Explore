{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f96fcb5",
   "metadata": {},
   "source": [
    "## **Plan 2 Explore**\n",
    "\n",
    "Model which learns not to chase but why to chase an unsupervised RL model uses the intrinsive reward signal for mastery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85ad0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import warnings\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d33b7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action = 'ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60396e8",
   "metadata": {},
   "source": [
    "#### **Device setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2340c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dca50",
   "metadata": {},
   "source": [
    "### **Env Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf89e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 348 | Action dim: 17 | Max action range: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Humanoid-v5', max_episode_steps = 1000)\n",
    "\n",
    "env = RescaleAction(env, min_action = -1.0, max_action = 1.0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "print(f'State dim: {state_dim} | Action dim: {action_dim} | Max action range: {max_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424576c3",
   "metadata": {},
   "source": [
    "### **Replay Buffer**\n",
    "\n",
    "This will be used by GRU powered Transformer(encoder - decoder style) to interact with env and save the trajectories in the buffer which will be used to train the SAC / PPO agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9f8d3",
   "metadata": {},
   "source": [
    "##### **Tensor Safe Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "84a9abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff6ee5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.pos = 0\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Safe conversion\n",
    "        \n",
    "        state = safe_tensor(state).to(device)\n",
    "        action = safe_tensor(action).to(device)\n",
    "        reward = safe_tensor(reward).to(device)\n",
    "        next_state = safe_tensor(next_state).to(device)\n",
    "        done = safe_tensor(done).to(device)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            \n",
    "            self.buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.buffer[self.pos] = experience\n",
    "            self.pos = (1 + self.pos) % self.capacity\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[ind] for ind in indices])\n",
    "        \n",
    "        # Safe conversion\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.buffer)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e13b5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagination_buffer = replay_buffer(capacity = 500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b457f",
   "metadata": {},
   "source": [
    "## **GRU Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9f759",
   "metadata": {},
   "source": [
    "#### **MLP Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d253cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, head_1, head_2, head_3, head_4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bfd22",
   "metadata": {},
   "source": [
    "### **GRU Powered Predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da22abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_GRU_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, h_1, h_2, h_3):\n",
    "        super(World_GRU_Model, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(latent_dim + action_dim, h_1, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(h_1, h_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(h_2, h_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h_3),\n",
    "            nn.Linear(h_3, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action, h = None):\n",
    "        \n",
    "        #print(f'Shape of state and action at gru model: {np.shape(state)} | {np.shape(action)}')\n",
    "        \n",
    "        if state.dim() == 4:  # [ensemble_size, batch, time_step, latent_dim]\n",
    "            \n",
    "            state = state.squeeze(0)  # Remove ensemble dimension\n",
    "            \n",
    "        #state = state.squeeze(3)    \n",
    "        \n",
    "        x = torch.cat([state, action], dim = -1)      # state and action are in shape [Batch, time_Step, latent_dim / action_dim] States are compressed for time step here\n",
    "        \n",
    "        out, h = self.gru(x, h)                 # h is past memory fed so that it does not start from 0\n",
    "        \n",
    "        pred_state = self.projection(out)\n",
    "        \n",
    "        return pred_state, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7f843",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73f19ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Encoder(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=348, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "World_GRU_Model(\n",
      "  (gru): GRU(145, 128, num_layers=2, batch_first=True)\n",
      "  (projection): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assembly\n",
    "\n",
    "latent_dim = 128\n",
    "head_1 = 256\n",
    "head_2 = 512\n",
    "head_3 = 256\n",
    "head_4 = 256\n",
    "\n",
    "h_1 = 128\n",
    "h_2 = 256\n",
    "h_3 = 512\n",
    "\n",
    "## Encoder\n",
    "\n",
    "encoder = Encoder(state_dim, latent_dim, head_1, head_2, head_3, head_4).to(device)\n",
    "\n",
    "print(\" - \" * 80)\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "# GRU World Model\n",
    "\n",
    "world_model = World_GRU_Model(latent_dim, action_dim, h_1, h_2, h_3).to(device)\n",
    "\n",
    "print(\" - \" * 80)\n",
    "\n",
    "print(world_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21577ede",
   "metadata": {},
   "source": [
    "### **World Model Loss**\n",
    "\n",
    "Here we will be using the RSS(Residual Squared) Formula which is \n",
    "\n",
    "**[*True Latent State - Pred Latent State*]^2**\n",
    "\n",
    "This will give an signal for world model to maximize uncertainity and dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bfd6b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_Model_Loss:\n",
    "    \n",
    "    def __init__(self, model, optimizer, scheduler, Encoder):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.encoder = Encoder\n",
    "        \n",
    "    def compute_loss(self,buffer, batch_size):\n",
    "        \n",
    "        states, actions, _, next_states, _ = buffer.sample(batch_size)\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "\n",
    "        #print(f\"Input shapes - states: {states.shape}, next_states: {next_states.shape}\")\n",
    "        \n",
    "        #latent_state = encoder(states)\n",
    "            \n",
    "        #latent_next_state = encoder(next_states)\n",
    "            \n",
    "        #print(f'Shape of latent next state: {latent_next_state.shape} | Latent state: {latent_state.shape}')    \n",
    "            \n",
    "        pred_latent_next_state, memory = self.model(states, actions)\n",
    "        \n",
    "        loss = F.mse_loss(next_states, pred_latent_next_state)\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 0.5)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "            \n",
    "        return loss, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c807cfa",
   "metadata": {},
   "source": [
    "### **Hyper Params and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13eb0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper params\n",
    "\n",
    "lr = 3e-4\n",
    "T_max = 1000\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer = optim.AdamW(world_model.parameters(), lr, weight_decay = 0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d431baa",
   "metadata": {},
   "source": [
    "## **Ensemble Dynamics**\n",
    "\n",
    "For disagreement intrinsic reward signal as more disagreement more signal to learn dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "07a007e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "    if isinstance(m, nn.Linear):\n",
    "        \n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "        if m.bias is not None:\n",
    "            \n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "975f4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_Dynamics(nn.Module):\n",
    "    \n",
    "    def __init__(self, ensemble_size, base_model = world_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ensemble = nn.ModuleList(\n",
    "            [copy.deepcopy(base_model) for _ in range(ensemble_size)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for model in self.ensemble:\n",
    "            \n",
    "            model.apply(init_weights)\n",
    "\n",
    "    def forward(self, latent_state, action):\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        for model in self.ensemble:\n",
    "            \n",
    "            pred, _ = model(latent_state, action)\n",
    "            \n",
    "            preds.append(pred)    \n",
    "        \n",
    "        return torch.stack(preds)   # Shape [ensmeble_size, batch_size, latent_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878c9bc",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b538e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble_Dynamics(\n",
      "  (ensemble): ModuleList(\n",
      "    (0-3): 4 x World_GRU_Model(\n",
      "      (gru): GRU(145, 128, num_layers=2, batch_first=True)\n",
      "      (projection): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (3): SiLU()\n",
      "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (5): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Size of ensemble\n",
    "\n",
    "ensemble_size = 4\n",
    "\n",
    "\n",
    "ensemble = Ensemble_Dynamics(ensemble_size)\n",
    "\n",
    "print(ensemble)\n",
    "\n",
    "# World loss setip\n",
    "\n",
    "world_loss = World_Model_Loss(world_model, optimizer, scheduler, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be5685",
   "metadata": {},
   "source": [
    "## **Intrinsive Reward**\n",
    "\n",
    "It will be learning signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4bf279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsive_reward_signal(latent_state):\n",
    "\n",
    "    disagreement = torch.var(latent_state, dim = 0)              # Compute variance across (batch, latent_state)\n",
    "    \n",
    "    intrinsive_reward = torch.mean(disagreement, dim = -1)   # Compute mean per batch\n",
    "    \n",
    "    return intrinsive_reward * 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a7ff3",
   "metadata": {},
   "source": [
    "# **SAC Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eef546",
   "metadata": {},
   "source": [
    "### **SAC BUFFER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11d858cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_BUFFER:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.pos = 0\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Safe conversion\n",
    "        \n",
    "        state = safe_tensor(state).to(device)\n",
    "        action = safe_tensor(action).to(device)\n",
    "        reward = safe_tensor(np.array(reward)).to(device)\n",
    "        next_state = safe_tensor(next_state).to(device)\n",
    "        done = safe_tensor(float(done)).to(device)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            \n",
    "            self.buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.buffer[self.pos] = experience\n",
    "            self.pos = (1 + self.pos) % self.capacity\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[ind] for ind in indices])\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones  = torch.stack(dones).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d31cb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_buffer = SAC_BUFFER(capacity = 500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2661a4",
   "metadata": {},
   "source": [
    "### **Agent Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d196bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, head):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Linear(input_dim, head),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(head, head),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(head, head),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head, head),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head, output_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.feature(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4903c",
   "metadata": {},
   "source": [
    "##### **Actor network**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbab83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, head_1, head_2, head_3, head_4, head, max_action = max_action):\n",
    "        super(Actor_Network, self).__init__()\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Pass to Feature network\n",
    "        \n",
    "        self.feature = Feature_Extractor(latent_dim, head_1, head)\n",
    "        \n",
    "        # Pass to norm\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # Pass norm to MHA\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(head_1, num_heads = 4, batch_first = True)\n",
    "        \n",
    "        # Pass to actor sequence\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # Pass state -> Feature extractor\n",
    "        \n",
    "        feature = self.feature(state)\n",
    "        \n",
    "        # Feature -> Norm\n",
    "        \n",
    "        norm = self.norm(feature)\n",
    "        \n",
    "        # Norm -> Unsqueeze(1) shape(batch, 1, embedd) -> Multi head attention\n",
    "        if norm.dim() == 2:\n",
    "            norm = norm.unsqueeze(1)  # [batch, 1, embed_dim]\n",
    "        \n",
    "        attn, _ = self.mha(norm, norm, norm)\n",
    "        \n",
    "        # Attn -> Squeeze(1) shape (batch, embedd) -> Actor sequence\n",
    "        \n",
    "        attn = attn.squeeze(1)\n",
    "        \n",
    "        x = self.actor(attn)\n",
    "        \n",
    "        # x -> mu (mean) and x -> log std\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        log_std = self.log_std(x)\n",
    "        \n",
    "        # Smooth scaling\n",
    "        \n",
    "        mu = torch.tanh_(mu)                # range [-1.0, 1.0]\n",
    "        log_std = torch.tanh_(log_std)      # range [-1.0, 1.0]\n",
    "        log_std = log_std.clamp(-10, 2)\n",
    "        std = torch.exp(log_std)\n",
    "                \n",
    "        # Reparameterization trick\n",
    "        \n",
    "        normal = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        z = normal.rsample()\n",
    "        \n",
    "        tanh_z = torch.tanh_(z)\n",
    "        \n",
    "        log_prob = normal.log_prob(z)\n",
    "        \n",
    "        action = self.max_action * tanh_z\n",
    "        \n",
    "        # Squashing\n",
    "        \n",
    "        squash = 2 * (torch.log(torch.tensor(2.0, device=z.device)) - z - F.softplus(-2 * z))\n",
    "        \n",
    "        log_prob = log_prob - squash\n",
    "        \n",
    "        log_prob = torch.sum(log_prob, dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bf8da",
   "metadata": {},
   "source": [
    "### **Critic Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0931922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, head_1, head_2, head_3, head_4, head):\n",
    "        super(Critic_Network, self).__init__()\n",
    "        \n",
    "        # Pass [state, action] -> Feature Extractor\n",
    "        \n",
    "        self.feature = Feature_Extractor(input_dim = latent_dim + action_dim, output_dim = head_1, head = head_1)\n",
    "        \n",
    "        # Norm and MHA\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(head_1, num_heads = 4, batch_first = True)\n",
    "        \n",
    "        # Critic network 1\n",
    "        \n",
    "        self.critic_1 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "        self.critic_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        cat = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        #print(f\"[Critic] state shape: {state.shape}, action shape: {action.shape}, cat: {cat.shape}\")\n",
    "\n",
    "        # cat -> Feature\n",
    "        \n",
    "        feature_1 = self.feature(cat)\n",
    "        feature_2 = self.feature(cat)\n",
    "        \n",
    "        # Feature -> Norm\n",
    "        \n",
    "        norm = self.norm(feature_1)\n",
    "        norm_2 = self.norm(feature_2)\n",
    "        \n",
    "        # Norm -> unsqueeze (batch, 1 , embedd) -> MHA\n",
    "        if norm.dim() == 2:\n",
    "            \n",
    "            norm = norm.unsqueeze(1)  # [batch, 1, embed_dim]\n",
    "        \n",
    "            norm_2 = norm_2.unsqueeze(1)\n",
    "        \n",
    "        attn_1, _ = self.mha(norm, norm, norm)\n",
    "        attn_2, _ = self.mha(norm_2, norm_2, norm_2)\n",
    "        \n",
    "        # Attn -> Squeeze (batch, embedd) -> Critic\n",
    "        \n",
    "        attn_1 = attn_1.squeeze(1)\n",
    "        attn_2 = attn_2.squeeze(1)\n",
    "        \n",
    "        q_1 = self.critic_1(attn_1)\n",
    "        q_2 = self.critic_2(attn_2)\n",
    "        \n",
    "        return q_1, q_2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c65d0",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1a877166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Actor_Network(\n",
      "  (feature): Feature_Extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): SiLU()\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (7): SiLU()\n",
      "      (8): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=256, out_features=17, bias=True)\n",
      "  (log_std): Linear(in_features=256, out_features=17, bias=True)\n",
      ")\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "Critic_Network(\n",
      "  (feature): Feature_Extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=145, out_features=256, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): SiLU()\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (7): SiLU()\n",
      "      (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (critic_1): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (critic_2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assembly\n",
    "\n",
    "head = 128\n",
    "latent_dim = 128\n",
    "\n",
    "# Actor network\n",
    "\n",
    "actor_network = Actor_Network(latent_dim, action_dim, head_1, head_2, head_3, head_4, head).to(device)\n",
    "\n",
    "print(\" - \" * 70)\n",
    "\n",
    "print(actor_network)\n",
    "\n",
    "# Critic network\n",
    "\n",
    "critic_network = Critic_Network(latent_dim, action_dim, head_1, head_2, head_3, head_4, head).to(device)\n",
    "\n",
    "print(\" - \" * 70)\n",
    "\n",
    "print(critic_network)\n",
    "\n",
    "# Target critic\n",
    "\n",
    "target_critic = copy.deepcopy(critic_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd94934",
   "metadata": {},
   "source": [
    "### *Soft Update*\n",
    "\n",
    "Softly updating critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32c58963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(source, target, tau = 0.067):\n",
    "    \n",
    "    for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "        \n",
    "        param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97fa3c",
   "metadata": {},
   "source": [
    "## **Agent Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "49145dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_AGENT:\n",
    "    \n",
    "    def __init__(self, actor, critic, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma, action_dim = action_dim, world_loss = world_loss):\n",
    "    \n",
    "        # Network\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.target_critic = target_critic\n",
    "        \n",
    "        # Actor \n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.actor_scheduler = actor_scheduler\n",
    "        \n",
    "        # Critic\n",
    "        \n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.critic_scheduler = critic_scheduler\n",
    "        \n",
    "        # Hyper param\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.world_loss = world_loss\n",
    "        \n",
    "        self.target_entropy = - action_dim * 2.5\n",
    "        self.log_alpha = torch.tensor(np.log(0.2), requires_grad = True, device = device)\n",
    "        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr = 1e-4, weight_decay = 0.001)\n",
    "        self.alpha_min = 0.1\n",
    "        \n",
    "    def compute_alpha(self):\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha = self.alpha.clamp(min = self.alpha_min, max = 0.2)\n",
    "        \n",
    "        return self.alpha\n",
    "    \n",
    "    def update_target(self):\n",
    "        \n",
    "        return soft_update(self.critic, self.target_critic)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \n",
    "        #print(f'Shape of state: {np.shape(state)}')\n",
    "        \n",
    "        action, _ = self.actor(state)\n",
    "        \n",
    "        return action, _ \n",
    "        \n",
    "    def update(self, replay_buffer,  batch_size):\n",
    "        \n",
    "        # Sample from buffer\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        \n",
    "        next_states = next_states.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        \n",
    "        # World Model Loss\n",
    "        \n",
    "        loss, memory = self.world_loss.compute_loss(replay_buffer, batch_size)\n",
    "        \n",
    "        # compute target val\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            new_action, _ = self.select_action(next_states)\n",
    "            target_1, target_2 = self.target_critic(next_states, new_action)    \n",
    "            target_q = (0.75 * torch.min(target_1, target_2) + 0.25 * torch.max(target_1, target_2))\n",
    "            \n",
    "            target = rewards + self.gamma * (1 - dones) * target_q\n",
    "            target = target.detach()\n",
    "        \n",
    "        # compute current target\n",
    "        \n",
    "        critic_1 , critic_2 = self.critic(states, actions)\n",
    "        loss_1 = F.smooth_l1_loss(critic_1, target)\n",
    "        loss_2 = F.smooth_l1_loss(critic_2, target)\n",
    "        \n",
    "        critic_loss = loss_1 + loss_2\n",
    "        \n",
    "        # Update Critics\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm = 0.5)\n",
    "        self.critic_optimizer.step()\n",
    "        self.critic_scheduler.step()\n",
    "        \n",
    "        \n",
    "        # Actor loss\n",
    "        \n",
    "        next_action, next_log_prob = self.select_action(states)\n",
    "        #log_prob = next_log_prob.detach()\n",
    "        \n",
    "        q_1, q_2 = self.critic(states, next_action)\n",
    "        q_pi = (0.75 * torch.min(q_1, q_2) + 0.25 * torch.max(q_1, q_2))\n",
    "        \n",
    "        actor_loss = (self.compute_alpha() * next_log_prob - q_pi).mean()\n",
    "            \n",
    "        # Update Actor\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm = 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "        self.actor_scheduler.step()\n",
    "        \n",
    "        # Alpha loss\n",
    "        \n",
    "        alpha_loss = - (self.log_alpha * (next_log_prob.detach() + self.target_entropy)).mean()\n",
    "        \n",
    "        # update alpha\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm = 0.5)\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # Update critics\n",
    "        \n",
    "        self.update_target()\n",
    "        \n",
    "        #print('Agent updated successfully in class')\n",
    "        return actor_loss.item(), critic_loss.item(), self.alpha.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "220fa901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper Params\n",
    "\n",
    "gamma = 0.997\n",
    "\n",
    "# Actor\n",
    "\n",
    "actor_optimizer = optim.AdamW(actor_network.parameters(), lr = 3e-4, weight_decay = 0.001)\n",
    "actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(actor_optimizer, T_max)\n",
    "\n",
    "# Critic\n",
    "\n",
    "critic_optimizer = optim.AdamW(critic_network.parameters(), lr = 3e-4, weight_decay = 0.001)\n",
    "critic_scheduler = optim.lr_scheduler.CosineAnnealingLR(critic_optimizer, T_max)\n",
    "\n",
    "# Agent Setuo\n",
    "\n",
    "agent = SAC_AGENT(actor_network, critic_network, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9a06",
   "metadata": {},
   "source": [
    "## **Mix Batch**\n",
    "\n",
    "Mixing Imaginary and Real batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1abaa3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix_Batch:\n",
    "    \n",
    "    def __init__(self, imagine_buffer, real_buffer, ratio):\n",
    "        \n",
    "        self.real_buffer = real_buffer\n",
    "        self.imagine_buffer = imagine_buffer\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        real_size = int(self.ratio * batch_size)\n",
    "        imag_size = batch_size - real_size\n",
    "\n",
    "        # Sample from buffers\n",
    "        \n",
    "        i_s, i_a, i_r, i_ns, i_d = self.imagine_buffer.sample(imag_size)\n",
    "        s, a, r, ns, d = self.real_buffer.sample(real_size)\n",
    "\n",
    "        # In mix_batch.sample():\n",
    "        \n",
    "        print(f\"Imaginary state shape: {i_s.shape}, Real state shape: {s.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Extract final transition from each rollout in imagination buffer\n",
    "        # Original: [imag_size, rollout_len, 1, feat_dim]\n",
    "        i_s  = i_s.squeeze(2)[:, -1, :]     # → [imag_size, feat_dim]\n",
    "        i_a  = i_a.squeeze(2)[:, -1, :]     # → [imag_size, act_dim]\n",
    "        i_r  = i_r.squeeze(2)[:, -1]        # → [imag_size]\n",
    "        i_ns = i_ns.squeeze(2)[:, -1, :]    # → [imag_size, feat_dim]\n",
    "        i_d  = i_d.squeeze(2)[:, -1]        # → [imag_size]\n",
    "\n",
    "        # Squeeze real data if needed\n",
    "        if s.dim() == 3:\n",
    "            s = s.squeeze(1)\n",
    "        if ns.dim() == 3:\n",
    "            ns = ns.squeeze(1)\n",
    "\n",
    "        # Encode real observations\n",
    "        s_encoded  = encoder(s.to(device))     # → [real_size, 128]\n",
    "        ns_encoded = encoder(ns.to(device))    # → [real_size, 128]\n",
    "\n",
    "        #print(f\"[After squeeze] i_s: {i_s.shape}, s: {s_encoded.shape}\")\n",
    "\n",
    "        # Merge real and imagined\n",
    "        states      = torch.cat([i_s.to(device), s_encoded], dim=0)\n",
    "        actions     = torch.cat([i_a.to(device), a.to(device)], dim=0)\n",
    "        rewards     = torch.cat([i_r.to(device), r.to(device)], dim=0)\n",
    "        next_states = torch.cat([i_ns.to(device), ns_encoded], dim=0)\n",
    "        dones       = torch.cat([i_d.to(device), d.to(device)], dim=0)\n",
    "\n",
    "        print(f\"Encoded real state shape: {s_encoded.shape}\")\n",
    "\n",
    "        # In world model training:\n",
    "        print(f\"Final mixed states shape: {states.shape}\")\n",
    "\n",
    "        #print(f\"[Final shapes] states: {states.shape}, actions: {actions.shape}, rewards: {rewards.shape}\")\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a67726c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_batch = Mix_Batch(imagination_buffer, sac_buffer, ratio = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb369dcc",
   "metadata": {},
   "source": [
    "## **Rollout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "558f4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_trajectories(start_state, agent = agent, ensemble = ensemble , imagination_buffer = imagination_buffer, horizon = 5):\n",
    "    \n",
    "    state = start_state\n",
    "    \n",
    "    for _ in range(horizon):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, _ = agent.select_action(state.squeeze(1))\n",
    "\n",
    "            action = action.unsqueeze(1)\n",
    "\n",
    "            ensemble_preds = ensemble(state, action)\n",
    "            \n",
    "            next_state = ensemble_preds.mean(dim = 0)\n",
    "            \n",
    "            reward = intrinsive_reward_signal(ensemble_preds)\n",
    "            \n",
    "            done = torch.zeros_like(reward)\n",
    "            \n",
    "            imagination_buffer.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state \n",
    "            \n",
    "            #print(f'Rollout succesfull')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
